---
title: "Workshop 11: Simulations in R"
subtitle: "QCBSx R Workshop Series <br> x = independent"
author: "Qu√©bec Centre for Biodiversity Science"
output:
  xaringan::moon_reader:
    includes:
      in_header: qcbsR-header.html
    lib_dir: assets
    seal: true
    css: ["default", "qcbsR.css", "qcbsR-fonts.css"]
    nature:
      beforeInit: "qcbsR-macros.js"
      highlightLines: true
---

<!-- --------------------- -->
<!-- NOTE TO THE PRESENTER -->
<!-- --------------------- -->

<!-- Presenter mode -->
<!-- 
Pressing "h" will show you important keyboard shortcuts
Select the presentation window and press "c" on the keyboard to "clone" the slides (the clones are in sync with each other) 
Click in one of the 2 presentation window, You can use the presenter mode by pressing "p" on the keyboard 
For more keyboard shortcuts, please visit 
https://bookdown.org/yihui/rmarkdown/xaringan-key.html 
-->

<!-- 
Math equations in Markdown 
https://rpruim.github.io/s341/S19/from-class/MathinRmd.html
-->

```{r setup, echo = F}
# in the end, should be about 1500 lines
knitr::opts_chunk$set(
  comment = "#",
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width=6, fig.height=6,
  fig.retina = 3,
  fig.align = 'center'
)
```

## Outline

1. Why are simulations relevant? 
2. What you need to know before doing simulations? 
3. Let's explore what underlie some statistical processes
3. Simulating data for power analysis (in development)
4. Simulating data for models (lm, glm, lmm, in development)

Please check out the [Simulation in R Cheat Sheet](https://docs.google.com/presentation/d/11bNgLUCyvlL8Ndq_nF-N6T71dZwgETbhbSHuc40VqvU/edit#slide=id.p)

???

This workshop was designed to give the tools for attendees to perform simulations. Simulations to get data in order to perform a statistical model, but also to be able to simulate a situation and verify how a parameter change the outcome of a model. 

---
## Prerequisites
<!-- I will be using base plot for this workshop: this is because it is MUCH EASIER to get quick BEAUTIFUL plots and show stuff -->
For this workshop it is useful to have a solid understanding of 

1. Linear models and generalized linear models (GLMs)
2. Programming in R 

See the [QCBS R workshops](https://r.qcbs.ca/) if you want to revise these topics. Select the [R workshops](http://r.qcbs.ca/workshops/) needed.

- We are going to review some aspects of these workshops, but getting some experience from those subjects can clearly help you for this workshop. 
- We are going to be more explicitly show assumptions of models and use for loops in this workshop 

???

This workshop builds on the previous workshops. But don't worry, we are going to refresh some of the concepts here. 

---
## Disclaimer

I am not a statistician and do not pretend to fully understand all the implications of what will be presented. 

In addition, this is a work in progress. Any input you have is more than welcome. 


---
## Learning objectives

1. Develop intuition and skills to perform simulations
2. Explore useful functions when designing simulations 
3. Articulate what are the assumptions underlying a simulation (for the models tested or the scope of the simulation)
4. Know how and when to use important statistical distributions
5. Simulate different data structure to perform LMs, GLMs, and (G)LMMs. 


###############
---
class: inverse, center, middle

# Why are simulations relevant?


---
## Simulations are games 

- Simulations are games that we make up the rules
- It equips us with a very powerful tool to create 'alternative worlds'. 
- The challenge in this game is to figure out what the parameters are for the processes existing in the natural world. 


.alert[Are these two distribution showing the same random process? ] 
```{r normal_compare_theoretical_simulated, echo=FALSE, fig.width=8, fig.height=4}
par(mfrow=c(1,2))
set.seed(12345)
curve(dnorm(x), from = -5,to = 5, ylim = c(0,1), ylab = "Density", lwd =3)
plot(density(rnorm(6)), ylim = c(0,1),main = "", xlim = c(-5,5), xlab = "x", lwd =3)
```


---
## Find patterns in random processes

- Simulations are useful to test the properties of randomly generated data 
- Since we designed the simulation, we know parameters of the processes that underlie it.
- It is then possible to test various methods to 
1. see if they work and verify their assumptions, 
2. do power analysis, 
3. learn how data is generated
4. etc. 

---
## Find patterns in the natural world

- The idea here is that perhaps we are interested in understanding how a certain process was generated (how $X$ influences $Y$, which could be written as $X \xrightarrow{Affects} Y$ or $Y \sim X$). 
- This is probably the most important points for simulations in biology as we are trying to understand **natural processes** (which are themselves manifestations of **random processes**)
- Therefore, being able to do simulations can be a nice addition to your research toolkit. 


---
## Probability distributions are crucial for simulations

- When performing simulations, once has to keep in mind the
1. **type of distribution** underlying the data of interest
2. **parameters** of the distribution itself (mean, standard deviation, rate, degrees of freedom, etc.)
3. **statistical model** making the relationship between the response variable and the explanatory variable. 
4. ideas or things we *want to show or learn* with the simulation

---
## Tips when performing simulation

.alert[Description section]. Add a 'Description' section to all your simulation scripts to introduce what the script is about. 

- You can add information how to use important arguments or a step-by-step description on how to use the script.
- You can add some references that you used to build your scrip. 

.small[
```r
# Description  ------------------------------------------------------------
#### ### ### ## #### ### ### ## #### ### ### ## 
# TITLE OF THE SCRIPT
# Created by YOUR_NAME
# 
# Why: 
# Requires:
# NOTES: 
# Reference : 
#### ### ### ## #### ### ### ## #### ### ### ## 

# Code here ... 

```
]


---
## Tips when performing simulation

.alert[Comments]. Be extra generous when commenting your code to describe as precisely as possible for the information that is not variable. 


.small[
```{r r_tips1, eval=FALSE}
# Description  ------------------------------------------------------------
## This is the description section as previously presented 

# Libraries ---------------------------------------------------------------
## Here load the libraries used in the script 
library(ggplot2)

# Functions ---------------------------------------------------------------
## Add and describe the functions used in the script 
## Add more information
function.name = function(var){tmp=2+var; return(tmp)}

# Plots -------------------------------------------------------------------
## Plotting the data simulated 
## Add more information
plot(function.name(1:10)+rnorm(10))
```
]

.small[
(We used fewer comments to lighten the presentation, but please add comments.)
]

###############
---
class: inverse, center, middle

# When could a simulation be useful? 

---
## ALL THE TIME!!!! 

Whether you want to 
- test a statistical method or a hypothesis, 
- learn how a mechanism work (i.e., genetic drift, natural selection), 
- play with data before your field work, 
- respond to reviewers for a paper you want to publish,
- etc. 

Simulations can help you at all stages of your research from planning to publishing and beyond! 

---
## Quick example: Genetic drift

- Here we have 4 plots showing genetic drift with the same simulation. 
- The only difference is the number of individuals in each population
- Your can deduce many things from the allele frequency change.

```{r r_tips2, echo=FALSE, fig.width=13,fig.height=7}
# Description  ------------------------------------------------------------
#### ### ### ## #### ### ### ## #### ### ### ## 
# Genetic drift simulation
# Created by Marc-Olivier Beausoleil
# 2022-01-07
# Why: 
# Requires:
# NOTES: 
# Drift is (from Futuyma)
# - unbiased
# - random fluctuations in allele frequency are larger in smaller populations
# - drift causes genetic variation to be lost
# - drift causes populations that are initially identical to become different
# - an allele can become fixed without the benefit of natural selection
# Reference : 
# Futuyma p. 167, figure 7.2
#### ### ### ## #### ### ### ## #### ### ### ## 

# graphing parameters -----------------------------------------------------
par(mfrow = c(2,2), cex = 1.2)
# Random seed -------------------------------------------------------------
set.seed(1245)

# Simulation parameters ---------------------------------------------------
# Number of gametes to chose from 
n.sperm = 2
n.eggs = 2
# Number of generations (x axis)
gen = 500
# Number of replicate populations 
popu = 5

# variance = p*(1-p)/(2*N)
# Variation is smaller when the population size is bigger 

# Loops -------------------------------------------------------------------
# Loops for all population replicates, tracking allele frequency change over the generations 
# number of individual per population 
n.id.pop = 5*10^seq(0,3, by=1)
# Loop that will change the maximum number of individual per population 
for (l in n.id.pop) {
  # Initial allele frequency 
  p.init = .5 
  # Maximum population size 
  max.pop = l
  # Total number of gametes in the population 
  n.gametes = c(max.pop*(n.sperm+n.eggs))
  # Make an empty object to record the population information 
  all.pops = NULL
  # Loop to track the all population allele frequency change 
  for (j in 1:popu) {
    all.fq.change = .5
    # Loop to track the within population allele frequency change 
    for (i in 1:gen) {
      # If the first iteration, make the probability equal the initial allele frequency 
      if (i == 1) {prob.p = p.init} else {prob.p = prop.all[2]}
      # binomial function to generate the new allele frequency (0 = q, 1 = p)
      allele.fq = rbinom(n = n.gametes, size = 1, prob = prob.p)
      # Randomly sample the population (this is the drift, a random sample of the population)
      all.drift = sample(x = allele.fq, size = max.pop, replace = F)
      # Get the proportion of the alleles in the new population 
      prop.all = prop.table(table(all.drift))
      # Record the p allele only 
      all.fq.change = c(all.fq.change, prop.all[2])
      # If there is an allele that goes to fixation, it'll print NA. In this case, break the for loop and go to the next iteration
      if(is.na(prop.all[2])) {break}
    } # End i
    # Record all population information
    one.pop = data.frame(p.fq = as.numeric(all.fq.change), pop = j)
    all.pops = rbind(all.pops,one.pop)
  } # End j
  
  
  # Remove all NAs ----------------------------------------------------------
  all.pops = na.omit(all.pops)
  
  
  # Plot --------------------------------------------------------------------
  # Make the empty plot 
  plot(all.pops$p.fq~c(1:nrow(all.pops)), 
       col = as.factor(all.pops$pop),
       main = paste0("Pop N=",max.pop, ", Start p=",p.init),
       ylab = "Allele frq p",
       xlab = "Generations",
       ylim = c(0,1),
       xlim = c(1,gen),
       type = "n")
  
  # Add the lines per population and colour them 
  for (k in 1:popu) {
    pttmp = all.pops[all.pops$pop==k,]
    points(c(1:nrow(pttmp)), pttmp$p.fq, 
           type = "l",
           col = k)
  } # End k
} # End l

```

---
## Quick (?) example: Natural selection 

- How would you model natural selection?

--

- What is the type of selection? 
- What is being selected? What is that type of data (continuous [range?], discrete [counts?])? 


```{r fake_fitness_functions, echo=FALSE, fig.width=8,fig.height=3}
# Fake fitness landscapes 
# Fitness test theoretical fitness landscapes 
par(mfrow=c(1,3))
q.fun = function(x, 
                 exponent = 2,
                 factor = -1,
                 xlab = xlab,
                 ylab = "Fitness",
                 lwd = 3,
                 ylim = c(0,4000),
                 cex.text = 1,
                 col.main = "black", 
                 col.line = "red", 
                 col.box = "black", 
                 col.text = "black") {
  y = x^exponent*factor
  plot(y~x, type = "l", 
       axes=FALSE,
       xaxs="i",yaxs="i",
       frame.plot=FALSE, 
       xlab="", ylab = "",lwd = lwd, col = col.main,
       # ylim = c(min(y),max(y)+5000)
       ylim = ylim)
  box(bty="l", lwd = 3)
  mtext(side=2,text=ylab,line = 1.1, col = col.text, cex = cex.text)
  mtext(side=1,text=xlab,line = 1.6, col = col.text, cex = cex.text)
  # axis(side = 1, labels = FALSE, tck = 0.000000001, lwd = 4, col = col.box)
  # axis(side = 2, labels = FALSE, tck = 0.000000001, lwd = 4, col = col.box)
  # y2 = -3*x^2+2500
  # points(y2~x, type = "l", lty = 2, lwd = lwd,
  #        col = col.line)
}
# col.main = "white"
col.main = "black"
xlab = ""
q.fun(-100:100,exponent = 1,factor = 10,xlab = "(Directional)",lwd = 5,cex.text = 2,
      ylab = "Fitness", col.line = NA,ylim = c(-1000, 2000),
      col.main = col.main,col.box = col.main, col.text = col.main)
q.fun(-100:100,xlab = "(Stabilizing)",lwd = 5,cex.text = 2,
      ylab = "", col.line = NA,ylim = c(-5000, 2000),
      col.main = col.main,col.box = col.main, col.text = col.main)
q.fun(-70:70,factor = 1,xlab = "(Disruptive)",lwd = 5,cex.text = 2,
      ylab = "", col.line = NA,ylim = c(-200, 7000),
      col.main = col.main,col.box = col.main, col.text = col.main)

```

--

- But what all of these mean? 
- What do they assume? 
- What is the data generated to make these lines and curves? 



###############
---
class: inverse, center, middle

# Let's build some simulation intuitions 

---
## What do you see here? 

- Can you give me *as much information* as you can from these graphs

```{r normalX_Y, echo=FALSE, fig.show='hide', fig.width=8,fig.height=4}
# source(file = "scripts/marginal_plot.R")
set.seed(123)
# par(mfrow = c(1,2), mar =c(4,4,3,3), cex = 1.4)
n = 250
x.1 = rnorm(n, mean = 15, sd = 5)
y.1 = 2*x.1 +rnorm(n, mean = 0, sd = 4)#rnorm(n, mean = 5, sd = 2)

x.2 = rnorm(n, mean = 15, sd = 1)
y.2 = 2*x.2 +rnorm(n, mean = 0, sd = .5) # rnorm(n, mean = 5, sd = .5)
# marginal_plot(x.1,y.1,ylim = range(c(y.1,y.2)), xlim = range(c(x.1,x.2)), pch = 19, col = scales::alpha("black",.8), ylab = "Y",xlab = "X")
# marginal_plot(x.2,y.2,ylim = range(c(y.1,y.2)), xlim = range(c(x.1,x.2)), pch = 19, col = scales::alpha("black",.8), ylab = "Y",xlab = "X")

library(ggplot2)
# Use base R here 
# https://stackoverflow.com/questions/71052975/how-to-plot-histograms-in-base-r-in-the-margin-of-a-plot?noredirect=1#comment125604177_71052975 
library(ggExtra)

size.line = .8
col.line = alpha("black",.5)
df.1 <- data.frame(x = x.1, y = y.1)
df.2 <- data.frame(x = x.2, y = y.2)
p.1 <- ggplot(df.1, aes(x, y)) + 
  geom_point() + 
  lims(x = range(c(x.1,x.2)), y = range(c(y.1,y.2))) +
  theme_classic() + 
  theme(axis.ticks = element_line(colour = "black"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 12, colour = "black"),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12))

p.2 <- ggplot(df.2, aes(x, y)) + 
  geom_point() + 
  lims(x = range(c(x.1,x.2)), y = range(c(y.1,y.2))) +
  theme_classic() + 
  theme(axis.ticks = element_line(colour = "black"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 12, colour = "black"),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12))
p.1
p.2

p.h1 <- p.1 + geom_hline(yintercept=mean(y.1), linetype="dashed", color = col.line, size=size.line)
p.h1
p.h.v1 <- p.h1 + geom_vline(xintercept=mean(x.1), linetype="dashed", color = col.line, size=size.line)
p.h.v1
p.h2 <- p.2 + geom_hline(yintercept=mean(y.2), linetype="dashed", color = col.line, size=size.line)
p.h2
p.h.v2 <- p.h2 + geom_vline(xintercept=mean(x.2), linetype="dashed", color = col.line, size=size.line)
p.h.v2

positions.1 <- data.frame(
  x = c(mean(x.1)-sd(x.1), mean(x.1)+sd(x.1), mean(x.1)+sd(x.1), mean(x.1)-sd(x.1)),
  y = c(mean(y.1)-sd(y.1), mean(y.1)-sd(y.1), mean(y.1)+sd(y.1), mean(y.1)+sd(y.1))
)
positions.2 <- data.frame(
  x = c(mean(x.2)-sd(x.2), mean(x.2)+sd(x.2), mean(x.2)+sd(x.2), mean(x.2)-sd(x.2)),
  y = c(mean(y.2)-sd(y.2), mean(y.2)-sd(y.2), mean(y.2)+sd(y.2), mean(y.2)+sd(y.2))
)

positions.x1 <- data.frame(
  x = c(mean(x.1)-sd(x.1), mean(x.1)+sd(x.1), mean(x.1)+sd(x.1), mean(x.1)-sd(x.1)),
  y = c(-Inf, -Inf, Inf, Inf)
)
positions.x2 <- data.frame(
  x = c(mean(x.2)-sd(x.2), mean(x.2)+sd(x.2), mean(x.2)+sd(x.2), mean(x.2)-sd(x.2)),
  y = c(-Inf, -Inf, Inf, Inf)
)
positions.y1 <- data.frame(
  x = c(Inf, -Inf, -Inf, Inf),
  y = c(mean(y.1)-sd(y.1), mean(y.1)-sd(y.1), mean(y.1)+sd(y.1), mean(y.1)+sd(y.1))
)
positions.y2 <- data.frame(
  x = c(Inf, -Inf, -Inf, Inf),
  y = c(mean(y.2)-sd(y.2), mean(y.2)-sd(y.2), mean(y.2)+sd(y.2), mean(y.2)+sd(y.2))
)
# p.h.v + geom_polygon(data = positions, aes(x = x, y = y),fill = alpha("gray70",.6))
p.h.v.sd.x1 = p.h.v1 + geom_polygon(data = positions.x1, aes(x = x, y = y),fill = alpha("gray70",.4))
p.h.v.sd.x1
p.h.v.sd.x2 = p.h.v2 + geom_polygon(data = positions.x2, aes(x = x, y = y),fill = alpha("gray70",.4))
p.h.v.sd.x2

p.h.v.sd.x.sd.y1 = p.h.v.sd.x1 + geom_polygon(data = positions.y1, aes(x = x, y = y),fill = alpha("gray70",.4))
p.h.v.sd.x.sd.y1
p.h.v.sd.x.sd.y2 = p.h.v.sd.x2 + geom_polygon(data = positions.y2, aes(x = x, y = y),fill = alpha("gray70",.4))
p.h.v.sd.x.sd.y2

p.h.v.sd.x.sd.y.l1 = p.h.v.sd.x.sd.y1 +  geom_smooth(method = "lm", se = TRUE, col =alpha("red",.5))
p.h.v.sd.x.sd.y.l1
p.h.v.sd.x.sd.y.l2 = p.h.v.sd.x.sd.y2 +  geom_smooth(method = "lm", se = TRUE, col =alpha("red",.5))
p.h.v.sd.x.sd.y.l2

p.marg1 = ggExtra::ggMarginal(p.h.v.sd.x.sd.y.l1, type = "histogram", fill = "gray80", col = "gray70")
p.marg1
p.marg2 = ggExtra::ggMarginal(p.h.v.sd.x.sd.y.l2, type = "histogram", fill = "gray80", col = "gray70")
p.marg2
# plot(y.1~x.1, ylim = range(c(y.1,y.2)), xlim = range(c(x.1,x.2)), pch = 19, col = scales::alpha("black",.8), ylab = "Y",xlab = "X")
# plot(y.2~x.2, ylim = range(c(y.1,y.2)), xlim = range(c(x.1,x.2)), pch = 19, col = scales::alpha("black",.8), ylab = "Y",xlab = "X")

```
.pull-left[
```{r normalX_Y1, echo=FALSE, fig.width=5,fig.height=4}
p.1
```
]

.pull-right[
```{r normalX_Y2, echo=FALSE, fig.width=5,fig.height=4}
p.2
```
]

Find correlation and covariance

- The correlation of the first graph is `r round(cor(x.1,y.1),2)` and the covariance is `r round(cov(x.1,y.1),2)`.
- The correlation of the second graph is `r round(cor(x.2,y.2),2)` and the covariance is `r round(cov(x.2,y.2),2)`.

---
## What do you see here? 

- Can you give me *as much information* as you can from these graphs

.pull-left[
```{r normalX_Y3, echo=FALSE, fig.width=5,fig.height=4}
# p.h1
p.h.v1
```
]

.pull-right[
```{r normalX_Y4, echo=FALSE, fig.width=5,fig.height=4}
# p.h2
p.h.v2

```
]

---
## What do you see here? 

- Can you give me *as much information* as you can from these graphs

.pull-left[
```{r normalX_Y5, echo=FALSE, fig.width=5,fig.height=4}
# p.h.v.sd.x1
p.h.v.sd.x.sd.y1
```
]

.pull-right[
```{r normalX_Y6, echo=FALSE, fig.width=5,fig.height=4}
# p.h.v.sd.x2
p.h.v.sd.x.sd.y2
```
]

---
## What do you see here? 

- Can you give me *as much information* as you can from these graphs

.pull-left[
```{r normalX_Y7, echo=FALSE, fig.width=5,fig.height=4}
p.h.v.sd.x.sd.y.l1
```
]

.pull-right[
```{r normalX_Y8, echo=FALSE, fig.width=5,fig.height=4}
p.h.v.sd.x.sd.y.l2
```
]

---
## What do you see here? 

- Can you give me *as much information* as you can from these graphs

.pull-left[
```{r normalX_Y9, echo=FALSE, fig.width=5,fig.height=4}
p.marg1
```
]

.pull-right[
```{r normalX_Y10, echo=FALSE, fig.width=5,fig.height=4}
p.marg2
```
]




###############
---
class: inverse, center, middle

# What you need to know before doing simulations? 


---
## Functions useful in simulations (RNG)

- When performing simulations, you will have to play with (pseudo-)randomly generated numbers from a random number generator (RNG). 
- This is a challenge if we want to replicate the analysis we are performing. 
- `R` has the function `set.seed()` that help us to play with the RNG.

The example below uses a RNG to extract numerical value between 1 and 10
```{r set.seed_function, echo=FALSE}
set.seed(123)
```

```{r runif_example}
runif(n = 1, min = 1, max = 10) # Gives a random number between 1 and 10
runif(n = 1, min = 1, max = 10) # RNG wasn't reset, different answer (see above)
runif(n = 1, min = 1, max = 10) # Different again... 

set.seed(42); runif(n = 1, min = 1, max = 10) # This sets the RNG 
set.seed(42); runif(n = 1, min = 1, max = 10) # The exact same number 

```

---
## Functions useful in simulations (sample)

- The function `sample()` randomly picks a value from a vector (i.e., random sampling).

The example below uses a RNG to extract numerical value between 1 and 10
```{r set.seed_hidden, echo=FALSE}
set.seed(123)
```

```{r sample_numerical_example}
set.seed(12) # Set the RNG 
v.1.10 = 1:10 # Make a vector from 1 to 10 
# Randomly pick 1 (size) value from the vector (x), without replacement 
sample(x = v.1.10, size = 1, replace = FALSE) 
```

- The values don't have to be numerical: they could be characters or factors

```{r sample_characters_example}
set.seed(3) # Set the RNG 
# Randomly pick 5 (size) letters from the vector (x), without replacement 
sample(x = LETTERS, size = 5, replace = FALSE) 
sample(x = as.factor(month.abb), size = 5, replace = FALSE) 
```

---
## Functions useful in simulations (sample)
<!-- SEE http://faculty.washington.edu/kenrice/sisg/sisg-lie11-05.pdf -->

- The function `sample()` can actually be used in order to do permutations 
- Let's say we have a data frame with 2 columns 

.verysmall[
.pull-left[
```{r permutations_load_viridis, echo=FALSE}
library(viridis)
```

```{r permutations_df, fig.width=4,fig.height=3}
set.seed(123)
n = 40; col = viridis::viridis(n = n)
x = 1:n ; y = 2+.5*x + rnorm(n, sd=7)
df.xy = data.frame(x,y, col )
```
]

.pull-right[
```{r permutations_XY, fig.width=4,fig.height=3}
set.seed(321)
df.xy$x.s = sample(df.xy$x)
df.xy$y.s = sample(df.xy$y)
# We break up the link of X and Y 
```
]

```{r permutations_plot, fig.width=9,fig.height=3}
par(mfrow=c(1,3), mar=c(4,4,1,1), cex = 1.2)
plot(y~x,  col=col, data=df.xy, pch=19);abline(lm(y~x,  data=df.xy)) # Original 
plot(y~x.s,col=col, data=df.xy, pch=19);abline(lm(y~x.s,data=df.xy)) # Permutated 
plot(y.s~x.s,col=col, data=df.xy, pch=19);abline(lm(y~x.s,data=df.xy)) # Permutated 
```
]

<!-- --- -->
<!-- ## Functions useful in simulations (sample) -->
<!-- - Keep in mind that permutations are valid if the null hypothesis tested is that there is *no association* between the variables studied.
http://faculty.washington.edu/kenrice/sisg/sisg-lie11-05.pdf
-->

---
## Functions useful in simulations (rep)

- You will not only see numerical values in simulations. Characters (or factors) can be generated in R easily. 
- The `rep()` function can help you with this

```{r rep_function_example}
(let4first = LETTERS[1:4])
rep(let4first, times = 2) # Repeat the WHOLE sequence twice 
rep(let4first, each = 2) # Repeat each element twice 
# Set the number of repeat for each element in the vector 
rep(let4first, times = c(1,2,3,6))
# Complete replication: replicate each element twice and do this three times 
rep(let4first, each = 2, times = 3)
rep(let4first, length.out = 6) # Repeat the vector until you hit the maximum length

```

---
## Functions useful in simulations

- In some cases, you want to replicate a process, for example when you want to generate multiple populations at once with the same parameters
- The `replicate()` can be used instead of a for loop and make simulations faster (I assume). 

```{r data_replicate}
set.seed(1234)
data.replicated = replicate(n = 2,expr = data.frame(gr = rep(LETTERS[1:3],each = 2),
                                                    y = rnorm(6)), 
                            simplify = FALSE)
```
.pull-left[
```{r data_replicate1}
data.replicated[[1]]
```
]
.pull-right[
```{r data_replicate2}
data.replicated[[2]]
```
]

---
## Challenge 1 ![:cube]()

Select randomly 4 values out of a vector of numbers ranging from 1 to 10 : 1. without replacement and 2. with replacement. 

Use `sample()` to perform this task. 

```{r show_sample, eval=FALSE}
sample()
```


---
## Challenge 1 - Solution ![:cube]()

Select randomly 4 values out of a vector of numbers ranging from 1 to 10 : 1. without replacement and 2. with replacement.

```{r sample_replacement}
set.seed(12345) # Sets the random number generator to a fix value
vec.1.10 = 1:10 # Make the vector to choose from 
sample(x = vec.1.10, size = 4, replace = FALSE) # Sample 4 nb without replacement
sample(x = vec.1.10, size = 4, replace = TRUE) # Sample 4 nb with replacement
```

As you can see in the last example, there are 2 "6"s, since each time a random number was picked, all numbers could be randomly chosen. 

1000 draws with replacement from 0 or 1, with equal probability for each. 
```{r sample_example}
set.seed(123); table(sample(x = 0:1, size = 1000, replace = T,prob = c(.5,.5)))
```

---
## Challenge 2 ![:cube]()

Create a data frame with variable x ranging from 1 to 10, $y = 2+ 3 * x$ and a grouping factor (gr) with one group with the 5 smallest values and another group with the 5 largest values. 

---
## Challenge 2 - Solution ![:cube]()
```{r linear_no_error, fig.width=8, fig.height=5}
x = 1:10
y = 2 + 3 * x
gr = rep(letters[1:2],each = 5)
linear.df = data.frame(x,y,gr)
plot(y~x, col = as.factor(gr), data = linear.df, pch = 19)
```


###############
---
class: inverse, center, middle

# Let's explore what underlies some statistical processes

---
## Probability and odds 

- When describing the chance of an outcome, you can use a probability or odds
- They are related: odds of an event is $odds = p/(1-p)$  or $odds = \frac{\text{probability of success}}{\text{probability of failure}}$
- Why bother? 
  - probability ranges between 0 and 1, 
  - odds range from 0 to $\infty$. 
  - Log (natural log, or ln) odds can range from $-\infty$ to $\infty$. It is calculated as $ln (odds) = ln(p/(1-p))$.
  - This will become handy when building a logistic model as this is the 'logit' transformation. 

<!-- .alert[Careful!] This is not the same as saying $success/(success + failure)$. This last calculation is the probability -->

---
## Probability and odds

- Example: a horse runs 100 races and wins 20 of them.  What is the *odd* of winning?
- $odds = p/(1-p)$  or $odds = \frac{\text{probability of success}}{\text{probability of failure}}$
  
```{r odds_example}
totl.nb.events = 100
successes = 20
odds.favor = (successes)/(totl.nb.events-successes) # Odds in favor of the event (here 1 in 4)
odds.against = (totl.nb.events-successes)/(successes) # Odds in against of the event (here 4 to 1)
library(MASS)
```

<!-- In the calculation above, I'm NOT using the probability of successes as the division by 100 (to make it a probability) would cancel out and make it harder to get a quick intuition from the toy example -->

So the horse wins 1 race for 4 fails or the odds of winning is `r fractions(odds.favor)`.
<!-- In other words, for 5 races the horse will win 1 and loose 4 -->

Conversely, the horse the odds of failing is `r paste0(odds.against,":1")` (read 4 *to* 1). 
<!-- In other words, the horse is 4 times more likely to fail than to succeed in a race. (???)  -->


---
## Probability and odds 

- Example
  - a horse runs 100 races and wins 20 of them. What is the *probability* of winning?
```{r probability_example}
totl.nb.events = 100
successes = 20
probability.favor = (successes)/(totl.nb.events) # probability of the event (here 20%)
probability.favor
fractions(probability.favor)
```
So the horse *wins* `r fractions(probability.favor)` or `r paste0(probability.favor*100,"%")` of the times. 

---
## Probability and odds 

```{r odds_prob, fig.width=9,fig.height=7.5}
log_odds = seq(from = -5, to = 5, by = .25)
odds = exp(log_odds)
inv.logit <- function(x) {exp(x)/(1 + exp(x))}
p = inv.logit(log_odds)
p2 = odds/(1 + odds)
# Probability of failure (1-p)
q = 1-p
# store log_odds and y in data frame for use with ggplot
d = data.frame(log_odds, odds, p, p2, q) 
head(d, 4)
```

<!-- Reference 
https://www.montana.edu/rotella/documents/502/Prob_odds_log-odds.pdf 
https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/
https://www.statisticshowto.com/probability-and-statistics/probability-main-index/odds-ratio/
-->
<!-- Good example on odds ration 
  https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_confidence_intervals/BS704_Confidence_Intervals10.html -->
  
---
## Probability and odds 

.tiny[
```{r odds_prob_plot, fig.width=10,fig.height=5.5}
par(mfrow = c(1,3), mar =c(4,4,1,1), cex = 1.4)
plot(d$odds~d$log_odds, type = "l", ylab = "Odds", xlab = "Log odds", lwd = 3) ; abline(v = 0, lty = 3)
plot(d$p~d$odds, type = "l", ylab = "p", xlab = "Odds", lwd = 3) ; abline(h = .5, v = 0, lty = 3)
plot(d$p~d$log_odds, type = "l", ylab = "p", xlab = "Ln odds", lwd = 3) ; abline(h = .5, v = 0, lty = 3)
```
]

---
## Why statistical distributions matter? 

- To help us in performing simulations, we need to get some data (random variables, X) which numerical values are outcomes of random processes. 
- The idea here is to find the distribution underlying a random process which generate a particular phenomenon in nature and use it to output values to be used in your simulation.
  - After generating the data, you can analyze it and see how it can be manipulated. 
  

---
## Functions for various distributions

- In `R`, there is a convention on how to call different distributions. 

| Type | Definition | Comment |
| ---- | ---------- | ------- |
| p	   | *probability* (*cumulative* distribution function or CDF; gives you the probability for a given quantile) | gives the probability that a random variable takes in a certain *range of values* for that random variable |
| q	   | *quantile* (inverse of CDF; gives you the x value for a given probability) | gives the position on the x axis where the area under the curve is a certain probability |
| d	   | *density* density function (probability density function or PDF) | gives probability that a random variables have for a specific value if the random variable  |
| r	   | *random* random variable coming from a certain distribution | We use this to generate random numbers from a specific distribution |

- In the next slide, you'll see some built in functions to use these distributions 

---
## Functions for various distributions

.verysmall[
| Distribution | Probability | Quantile | Density | Random | PDF or PMF |
| ------------ | ----------- | -------- | ------- | ------ | ---------- |
| Beta	       | `pbeta`	 | `qbeta`	| `dbeta`	| `rbeta` | |
| Binomial	   | `pbinom`	 | `qbinom`	| `dbinom`	| `rbinom` | $f(x) = {n \choose x} p^x(1-p)^{n-x}$ |
| Cauchy       | `pcauchy` | `qcauchy` | `dcauchy`	| `rcauchy` ||
| Chi-Square	 | `pchisq`	 | `qchisq`	 | `dchisq`	| `rchisq` | |
| Exponential	 | `pexp`    | `qexp`	 | `dexp`	| `rexp` ||
| F	           | `pf`	     | `qf`	| `df`	| `rf` | |
| Gamma	       | `pgamma`	 | `qgamma`	| `dgamma`	| `rgamma` | |
| Geometric	   | `pgeom`	 | `qgeom`	| `dgeom`	| `rgeom` | |
| Hypergeometric | `phyper`	| `qhyper`	| `dhyper`	| `rhyper` ||
| Logistic	     | `plogis`	| `qlogis`	| `dlogis`	| `rlogis` | |
| Log Normal	   | `plnorm`	| `qlnorm`	| `dlnorm`	| `rlnorm` | |
| Negative Binomial	| `pnbinom`	| `qnbinom`	| `dnbinom`	| `rnbinom` | |
| Normal	     | `pnorm`	| `qnorm`	| `dnorm`	| `rnorm` | $f(x)={\frac{1}{\sigma\sqrt{2\pi}}}e^{-{\frac{1}{2}}\Bigl(\frac {x-\mu}{\sigma}\Bigr)^2}$ |
| Poisson	     | `ppois`	| `qpois`	| `dpois`	| `rpois` | $f(x) = e^{-\lambda}\frac{\lambda^x}{x!}$ |
| Student t	   | `pt`	| `qt`	| `dt`	| `rt` ||
| Studentized Range | `ptukey`	| `qtukey`	| `dtukey`	| `rtukey` | |
| Uniform	     | `punif`	| `qunif`	| `dunif`	| `runif` | $f(x) = \frac{1}{b-a}$ for $x \in [a,b]$, $0$ otherwise |
| Weibull	     | `pweibull`	| `qweibull`	| `dweibull`	| `rweibull` ||
| Wilcoxon Rank Sum Statistic	   | `pwilcox`	| `qwilcox`	| `dwilcox`	| `rwilcox` ||
| Wilcoxon Signed Rank Statistic | `psignrank`	| `qsignrank`	| `dsignrank`	| `rsignrank` ||
]

---
## Functions for various distributions

.verysmall[
| Distribution | Probability | Quantile | Density | Random | PDF or PMF |
| ------------ | ----------- | -------- | ------- | ------ | ---------- |
| Beta	       | `pbeta`	 | `qbeta`	| `dbeta`	| `rbeta` ||
| <span style="background-color: #FFFF00">**Binomial**</span>	   | `pbinom`	 | `qbinom`	| `dbinom`	| `rbinom` | $f(x) = {n \choose x} p^x(1-p)^{n-x}$ |
| Cauchy       | `pcauchy` | `qcauchy` | `dcauchy`	| `rcauchy` ||
| <span style="background-color: #FFFF00">**Chi-Square**</span>	 | `pchisq`	 | `qchisq`	 | `dchisq`	| `rchisq` ||
| <span style="background-color: #FFFF00">**Exponential**</span>	 | `pexp`    | `qexp`	 | `dexp`	| `rexp` ||
| <span style="background-color: #FFFF00">**F**</span>	           | `pf`	     | `qf`	| `df`	| `rf` ||
| Gamma	       | `pgamma`	 | `qgamma`	| `dgamma`	| `rgamma` ||
| Geometric	   | `pgeom`	 | `qgeom`	| `dgeom`	| `rgeom` ||
| Hypergeometric | `phyper`	| `qhyper`	| `dhyper`	| `rhyper` ||
| <span style="background-color: #FFFF00">**Logistic**</span>	     | `plogis`	| `qlogis`	| `dlogis`	| `rlogis` ||
| <span style="background-color: #FFFF00">**Log Normal**</span>	   | `plnorm`	| `qlnorm`	| `dlnorm`	| `rlnorm` ||
| Negative Binomial	| `pnbinom`	| `qnbinom`	| `dnbinom`	| `rnbinom` ||
| <span style="background-color: #FFFF00">**Normal**</span>	     | `pnorm`	| `qnorm`	| `dnorm`	| `rnorm` | $f(x)={\frac{1}{\sigma\sqrt{2\pi}}}e^{-{\frac{1}{2}}\Bigl(\frac {x-\mu}{\sigma}\Bigr)^2}$ |
| <span style="background-color: #FFFF00">**Poisson**</span>	     | `ppois`	| `qpois`	| `dpois`	| `rpois` | $f(x) = e^{-\lambda}\frac{\lambda^x}{x!}$ |
| <span style="background-color: #FFFF00">**Student t**</span>	   | `pt`	| `qt`	| `dt`	| `rt` ||
| Studentized Range | `ptukey`	| `qtukey`	| `dtukey`	| `rtukey` ||
| <span style="background-color: #FFFF00">**Uniform**</span>	     | `punif`	| `qunif`	| `dunif`	| `runif` | $f(x) = \frac{1}{b-a}$ for $x \in [a,b]$, $0$ otherwise |
| Weibull	     | `pweibull`	| `qweibull`	| `dweibull`	| `rweibull` ||
| Wilcoxon Rank Sum Statistic	   | `pwilcox`	| `qwilcox`	| `dwilcox`	| `rwilcox` ||
| Wilcoxon Signed Rank Statistic | `psignrank`	| `qsignrank`	| `dsignrank`	| `rsignrank` ||
]

<!-- 
PDF for continuous 
PMF for discrete 
# See also how to convert them from one another
https://en.wikipedia.org/wiki/Relationships_among_probability_distributions
-->

---
## Common statistical distributions
<!-- ![:scale 90%](images/distributions_all.png) -->

```{r Statistical_dist, echo=FALSE, out.width=750, out.height=600, fig.width=9,fig.height=7.5}
par(mfrow = c(3,3), bg = NA)
# par(mfrow = c(1,1), bg = NA)
col = scales::alpha("black",.5)
col.r = scales::alpha("red",.5)
col.g = scales::alpha("green",.5)
col.b = scales::alpha("blue",.5)
lwd=2
# Binom
#define range of "successes"
success <- 0:20
plot(success, dbinom(x = success, size=20, prob=0.4),type='h', col = col.b, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Binomial")
curve(expr = dbinom(x = x, size=20, prob=0.6),col = col, lwd=lwd, ylim = c(0,1), type = 'h',add = T)
curve(expr = dbinom(x = x, size=1, prob=0.5),col = col.r, lwd=lwd, ylim = c(0,1), type = 'h',add = T)
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)
legend("topright",legend = c("n = 20, p = .4","n = 20, p = .6","n = 1, p = .5"),lty = c(1,1,1), col =c(col.b,col, col.r), lwd = 2)

# Logistic
curve(dlogis(x,location = 0, scale = 1), from=-10, to=10, col = col, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Logistic, l = 0, s = 1")
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

# Chi-sq
curve(dchisq(x, df = 10), from = 0, to = 40, col = col.b, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Chi-square")
curve(dchisq(x, df = 4), from = 0, to = 40, col = col, lwd=lwd, ylim = c(0,1), add = T)
curve(dchisq(x, df = 1), from = 0, to = 40, col = col.r, lwd=lwd, ylim = c(0,1), add = T)
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)
legend("topright",legend = c("df = 10","df = 4","df = 1"),lty = c(1,1,1), col =c(col.b,col, col.r), lwd = 2)

# Exponential 
curve(dexp(x, rate = .5), from=0, to=10, col=col.b, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Exponential")
curve(dexp(x, rate = .2), from=0, to=10, col=col, lwd=lwd, ylim = c(0,1),add = T)
curve(dexp(x, rate = .8), from=0, to=10, col=col.r, lwd=lwd, ylim = c(0,1),add = T)
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)
legend("topright",legend = c("rate = 0.8","rate = 0.5","rate = 0.2"),lty = c(1,1,1), col =c(col.b,col,col.r), lwd = 2)

# F-distribution
curve(df(x, df1 = 10, df2 = 20), from = 0, to = 4, n = 5000, col= col, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "F-distribution, df1 = 10, df2 = 20")
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

# normal 
curve(expr = dnorm(x = x, mean=0,sd=1), from = -5, to = 5, col=col.b, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Normal")
curve(expr = dnorm(x = x, mean=0,sd=2), from = -5, to = 5, col=col, lwd=lwd, ylim = c(0,1), add = T)
curve(expr = dnorm(x = x, mean=2,sd=1), from = -5, to = 5, col=col.r, lwd=lwd, ylim = c(0,1), add = T)
curve(dt(x, df=1), from=-5, to=5, col=col.g, lty = 1, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "t-distribution df = 10", add=TRUE)
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)
legend("topright",legend = c("Normal, m = 0, sd = 1","Normal, m = 0, sd = 2","Normal, m = 2, sd = 1","t-distribution, df =1"),lty = c(1,1,1,1), col =c(col.b,col,col.r,col.g), lwd = 2)

# Log normal 
curve(dlnorm(x, meanlog=0, sdlog=1), from=0, to=10, col=col, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Log-normal, m = 0 sd = 1")
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

# Poisson
plot(success, dpois(success, lambda=5), type='h', col=col, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Poisson, lambda = 5")
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

# unifrom
curve(dunif(x, min = 8,max = 9), from=5, to=25, col=col.b, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Uniform") 
curve(dunif(x, min = 10,max = 15), from=5, to=25, col=col, lwd=lwd, ylim = c(0,1), add=T) 
curve(dunif(x, min = 16,max = 18), from=5, to=25, col=col.r, lwd=lwd, ylim = c(0,1), add = T) 
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)
legend("topright",legend = c("min = 8, max = 9","min = 10, max = 15","min = 16, max = 18"),lty = c(1,1,1), col =c(col.b,col,col.r), lwd = 2)
```

---
## Summary of important distribution 

.small[
- Distributions of random variables comes in 2 flavors: 
1. Discrete (only *countable* number of *distinct* values, e.g., 0, 5, 19,...)
2. Continuous (infinite number of possible values, e.g., 0.1,0.62113,...)


| Distribution | Definition | Type of data | Example |
| ------------ | ---------- | ------------ | ------------ |
| Binomial	   | probability of *n* repeated *yes/no* experiments with probability *p* of *success* | discrete; yes/no; 0/1 | Number of heads in a row after 3 tosses |
| Poisson	     | probability of a number of cases happening in a defined range of time or space given a constant rate | discrete; | ticks of a radiation counter, DNA mutation, colonization of animals on remote islands (something per unit time or space for example) |
| Logistic	   | probability of an individual to be in one class or another | continuous; | alive or not, yes or no, diseased or not, etc. |
| Normal	     | probability function which informs how values are distributed | continuous; | height of individuals, beak length, shoe size, time required to run a kilometer, the data shows a 'bell-shaped' curve |
| Uniform	     | probability of all outcomes are equal | continuous; | angle between 0-360 | 
<!-- | Exponential	 | | continuous;| Generating genealofies | -->
<!-- | Student t	   |            | continuous; | | -->
<!-- | Geometric	   |            | continuous; | Gene conversion (recombination) | -->
<!-- | Beta	       |            |              | -->
<!-- | Chi-Square	 |            |              | -->
<!-- | F	           |            |              | -->
]

---
## The same distribution? 

.alert[Careful!]

- Some distribution are similar but don't show the same process 

```{r similar_plot_from_different_distributions, echo=FALSE}
par(mfrow = c(1,2))
set.seed(1234)
hist(rbinom(10000, 10, 0.5), #breaks = seq(-0.5, 10.5, by = 1), 
     main = "Binomial")
hist(rnorm(10000, 5, 1.5), xlim = c(0,10), main = "Normal")
```


---
## Other distributions with packages

- Other packages have other distributions. See the [CRAN Task View: Probability Distributions](https://cran.r-project.org/web/views/Distributions.html) if you need a funky distribution that doesn't come in `base R`.
- Some distributions can be recreated in R even if they haven't their own functino name (e.g., Bernoulli). 
  - Bernoulli distribution can be simulated with a binomial distribution with `size = 1`.

.tiny[
```{r equivalence_between_distributions_Bern_binom, echo=FALSE, fig.width=7,fig.height=4}
par(mfrow = c(1,2), cex = 1.4)
# install.packages("Rlab")
library("Rlab")
# Bernoulli 
x <- seq(0, 1, by = 1) 
y_dbern <- dbern(x, prob = 0.7)
barplot(height = y_dbern,
        names.arg = setNames(c(.3, .7), c('absent (0)', 'present (1)')), 
        ylim = c(0, 1), xlab = '', ylab = 'probability', main = 'Bernoulli p = 0.7')

# Binomial (to get Bernoulli) 
x <- seq(0, 1, by = 1) 
y_binom <- dbinom(x,size = 1, prob = 0.7)
barplot(height = y_binom,
        names.arg = setNames(c(.3, .7), c('absent (0)', 'present (1)')), 
        ylim = c(0, 1), xlab = '', ylab = 'probability', main = 'Binomial, n = 1, p = 0.7')

# Simulate 10 (fair) coin flips
# set.seed(98765)
# rbinom(n = 10, size = 1, prob = 0.5)
```
]

```{r simulate_coin_flips_plot, echo=FALSE, eval=FALSE}
# Coin flips visualize
set.seed(1235)

nb.flips = 10
set.seed(98765)
coin.flips = rbinom(n = nb.flips, size = 1, prob = 0.5)
heads.tails = ifelse(coin.flips==1,"H","T")
radius =1 
# initialize a plot
plot(x = c(0, nb.flips*(radius+.5)), y = c(-.5, 1), type = "n", 
     axes = F,
     asp = 1, 
     ylab = "", xlab = "")
w = 0
pos=0
for (i in 1:nb.flips) {
  if (i %% 5 == 0) {
    pos = pos + 2.5
    w = 0
  }
   
 # prepare "circle data"
  radius = 1
  center_x = w + 1
  center_y = pos
  theta = seq(0, 2 * pi, length = 200) # angles for drawing points around the circle
  
  # draw the circle
  lines(x = radius * cos(theta) + center_x, y = radius * sin(theta) + center_y)
  polygon(x = radius * cos(theta) + center_x, y = radius * sin(theta) + center_y, col = scales::alpha("beige",.5))
  text(center_x,center_y,labels = heads.tails[i], cex = 3)
  w = w + 2*radius+.5
}

```



---
## Poisson distribution

- Poisson distribution assume that 
  - the probability of some event is small over a short period of time (or area), but that there are a lot of events (as there would be a lot of time passing, or a lot of space) in which the process could happen 
  - The number of event reported is independent from one time point to the other. 
  - In summary, it models *rare* events that happen at a certain *rate*. In other words, there are many opportunities to succeed (n is large), but the probability of success (for each trial) is small
  
  $$f(x) = e^{-\lambda}\frac{\lambda^x}{x!}$$ where $\mu = \sigma = \lambda$
  

---
## Poisson distribution vs binomial

- Difference with Binomial :
  - Number of events in Poisson can be infinite (so n tends towards infinity and the probability of each event approaches 0)
  - Number of trials is finite ( $n$ ) in Binomial distribution 
  
```{r equivalence_between_distributions_Binom_poisson, echo=FALSE}
par(mfrow = c(2,2))
x <- 0:10
n <- 10000
barplot(dbinom(x, n, 2/n), names.arg = x, ylim = c(0, 0.35), main = paste("Binomial with n = ", n))
barplot(dbinom(x, n, 9/n), names.arg = x, ylim = c(0, 0.35), main = paste("Binomial with n = ", n))
barplot(dpois(x, 2), names.arg = x, ylim = c(0, 0.35), main = paste("Poisson with Lambda = ", 2))
barplot(dpois(x, 9), names.arg = x, ylim = c(0, 0.35), main = paste("Poisson with Lambda = ", 9))

pbinom(q = 2, size = n, prob = 2/n)
ppois(q = 2, lambda = 2)
```


---
## Poisson distribution

- Examples (something per unit time or space for example): 
  - Nb of deaths due to a disease (e.g., SARS-CoV 2) for a period of time 
  - Ticks of a radiation counter (Geiger counter which detects and measures ionizing radiation)
  - DNA mutation
  - Colonization of animals on remote islands
  - Mutation models
  - Recombination models 
  - Bacterial colony growing on an agar plate 
  
<!-- Examples taken from
https://ani.stat.fsu.edu/~debdeep/p4_s14.pdf
-->
  
```{r petri_dish_bacteria_colonny, echo=FALSE, fig.width=5,fig.height=3}
set.seed(1235)
# initialize a plot
plot(c(-1, 3.5), c(-1, 1), type = "n", axes = F, asp = 1, ylab = "", xlab = "")
n = 25
# prepare "circle data"
radius = 1
center_x = 0
center_y = 0
theta = seq(0, 2 * pi, length = 200) # angles for drawing points around the circle

# draw the circle
lines(x = radius * cos(theta) + center_x, y = radius * sin(theta) + center_y)
polygon(x = radius * cos(theta) + center_x, y = radius * sin(theta) + center_y, col = scales::alpha("beige",.5))

center_x = 2.5
center_y = 0
lines(x = radius * cos(theta) + center_x, y = radius * sin(theta) + center_y)
polygon(x = radius * cos(theta) + center_x, y = radius * sin(theta) + center_y, col = scales::alpha("beige",.5))

rdmunif<-runif(n,0,1)
r = runif(n,0,radius) #radius * sqrt(rdmunif)
theta = rdmunif * 2 * pi
x = center_x + sqrt(r*radius) * cos(theta)
y = center_y + sqrt(r*radius) * sin(theta)

points(x,y, pch =19, cex = .6)

arrows(1.1,0,1.4,0, length = .1)
# draw sq
x.pos = c(1.9,2.5, 2.9,2,2.2)
y.pos = c(0,.3,-.6,.3,-.8)
dx = .1
for (i in 1:length(x.pos)) {
  polygon(x = c(x.pos[i],x.pos[i]+dx,x.pos[i]+dx,x.pos[i]),y = c(y.pos[i],y.pos[i],y.pos[i]-dx,y.pos[i]-dx))
}
```

<!-- Useful resource -->
<!-- Poisson definition see https://ani.stat.fsu.edu/~debdeep/p4_s14.pdf -->
<!-- Poisson example see https://www.pnas.org/content/115/37/9270 -->

---
## Binomial distribution

- Binomial distribution assume that 
  - Each *independent* trial is either a success (1, T, Yes) or failure (0, F, No)
    * Note that a 'success' or 'failure' is a *contrast* between 2 elements. 
    * These could be 'heads/tail', 'girl/boy', 'diseased/not', etc.  
  - There is a *fixed* number of trials and the probability on each trial is constant.

<!-- , 'cancer cell death/alive' -->


$$f(x) = {n \choose x} p^x(1-p)^{n-x} $$
.tiny[
where 
- $p$ is the probability of success in a single trial (therefore, $(1-p)$ is the probability of failure, sometimes seen as $q$)
- $n$ is the number of trials 
- $x$ is the number of trials that are successes
- ${n \choose x}$ is the number of combinations (which is equal to $\frac{n!}{r!(n-r)!}$) where the order doesn't matter. 
]

---
## Binomial distribution

- Examples 
  - Number of heads in a row after 3 tosses
  - Genetic drift 
  - Wright-Fisher model 

<!-- Examples taken from 
http://www.biology.arizona.edu/biomath/tutorials/polynomial/applications/BinomialEx3.html
https://www.zoology.ubc.ca/~bio300b/binomialnotes.html
https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/bs704_probability7.html
-->

---
## Binomial distribution

- Examples 
  - probability of having a certain number of offspring given the genotypes of the parents 
    - for example, if a male is AA and female Aa, what is the probability that 6 of their 7 offspring has Aa
      - Find the probability of getting Aa from AA x Aa [1/2 AA and 1/2 Aa], 
      - then out of n=7 offspring from that couple, 6 are of genotype Aa, $P(6|7) = {7 \choose 6} (1/2)^6(1-1/2)^{7-6} = 7/128$)
  - Probability of a certain number of your children being daughters 
    - if you have 5 children, these are independent trials, so n = 5
    - "success" is the probability that the child is a daughter (p = 1/2)
    - $P(3|5) = {5 \choose 3} (1/2)^3(1-1/2)^{5-3} = 5/16$


---
## Normal distribution (Gaussian)

- This distribution is the most important to know (see Central limit theorem). 
- It has interesting properties (mean: center, sd: spread, bell-shaped, symmetry, etc.)
- Can approximate other distribution (might require data transformation)
- The normal distribution with mean = 0 and sd = 1 is called the **Standard Normal**

The general form of the probability density function for a normal distribution is 

$$f(x)={\frac{1}{\sigma\sqrt{2\pi}}}e^{-{\frac{1}{2}}\Bigl(\frac {x-\mu}{\sigma}\Bigr)^2}$$

The important part here is the mean $\mu$ and the standard deviation $\sigma$. Usually summarized as $N(\mu, \sigma)$

<!-- Material inspiration
https://ani.stat.fsu.edu/~debdeep/p4_s14.pdf
-->

---
## Normal distribution (Gaussian)

- Examples: 
  - Distribution of a lot of continuous phenotypes 
    - Arm length, or human height in a population
  - velocity of a collection of molecules in a gas of liquid
  - the error from measurements (length of beak).


---
## Normal distribution (Gaussian)

- The `iris` dataset contains values that are 'drawn' from a normal distribution. 
- Here are the histogram for sepal length (mm).

```{r Iris_normal, echo=FALSE, fig.width=11,fig.height=5}
par(mfrow = c(1,3), cex = 1.4)
spiris = unique(iris$Species)
for (i in 1:length(spiris)) {
  tmp.iris=iris[iris$Species %in% spiris[i],]
  hist(tmp.iris$Sepal.Length,
       main = paste("Iris",spiris[i]),
       xlab = "Sepal length",
       xlim = c(3.8,9), breaks = 10)
}


```


---
## Normal distribution (dissection)

.pull-left[
```{r Density_normal, fig.width=5,fig.height=5}
curve(expr = dnorm(x = x, mean=0,sd=1), 
      from = -5, to = 5, ylim = c(0,1),
      col="black", ylab = "Density",
      main = "Density normal") 
abline(h=seq(0,1, by = .1), 
       lty = 3, lwd = .3)

```
]

--

.pull-right[
For the density distribution, the *area* under the curve between two points is the *probability*. For example, the 95% probability (red region), and 2 tails 2.5% (blue region).

```{r Density_normal_dissection_all_info, echo=FALSE, fig.width=5,fig.height=5}
x <- seq(-5, 5, 0.1)
cex  = .7
plot(x, dnorm(x, 0, 1), main = "Density normal", type = "l", lwd = 3, col = "black", ylab = "", xlab = "x", ylim = c(0,1))
# quantile.normal = qnorm(seq(0,1, by = .05))
quantile.normal = qnorm(c(.999,0.001,.975,.025, .95, .05, .9, .1))
abline(v = quantile.normal[!is.infinite(quantile.normal)], lty = 3, lwd = .3) 
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

p = 0.025 

# add the polygon to the left  
lb <- min(x) # Lower bound
ub <- qnorm(p)   # Upper bound
x2 <- seq(min(x), ub, length = 100) # New Grid
y <- dnorm(x2, 0, 1) # Densitypolygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
polygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
text(x = ub, y = p+0.04,labels = paste0("pnorm(qnorm(p)) = \n",p*100,"%"),adj = 0,pos = 2, cex=cex)

text(x = ub-.6, y = .2,labels = paste0("qnorm(p) = ", round(qnorm(p),2)),adj = 0,pos = 1,offset = -1, cex=cex)
arrows(x0 = ub,x1 = ub, y0 = .2,y1 = .1,code = 2,length=.1)


# add the polygon to the right 
lb <- qnorm(1-p) # Lower bound
ub <- max(x)   # Upper bound
x2 <- seq(lb, max(x), length = 100) # New Grid
y <- dnorm(x2, 0, 1) # Density
polygon(c(lb, x2, ub), c(0,y,0), col = rgb(0, 0, 1, alpha = 0.5))
text(x = lb, y = p+0.04,labels = paste0(p*100,"%"),adj = 0,pos = 4, cex=cex)

text(x = lb+.7, y = .2,labels = paste0(round(qnorm(1-p),2)," = qnorm(1-p)"),adj = 0,pos = 1,offset = -1, cex=cex)
arrows(x0 = lb,x1 = lb, y0 = .2,y1 = .1,code = 2,length=.1)

# Add the middle (red) polygon 
lb <- qnorm(p) # Lower bound
ub <- qnorm(1-p)   # Upper bound
x2 <- seq(lb, ub, length = 100) # New Grid
y <- dnorm(x2, 0, 1) # Density
polygon(c(lb, x2, ub), c(0,y,0), col = rgb(1, 0, 0, alpha = 0.5))
text(x = mean(x2), y = .2,labels = paste0((1-2*p)*100,"% \n=", "\npnorm(qnorm(1-p)) - \npnorm(qnorm(p))"),adj = 0,pos = 1, cex=cex)

text(x = 0, y = .55,labels = paste0(quote("Mean")),adj = 0,pos = 1,offset = 0, cex=cex)
arrows(x0 = 0,x1 = 0, y0 = .5,y1 = .42,code = 2,length=.1)

legend("topright",legend = c("Density normal (dnorm)","Density rnorm(100)"), lwd = 1, lty = 1, col = c("black", "green"))

set.seed(123)
rndat =rnorm(100)
# mean(rndat)
dens.nor = density(rndat)
lines(dens.nor, lwd = 3,col = scales::alpha("green",.8))
mybins=hist(rndat, plot = F, density = T, breaks = 100)
crn = mybins$density
brn = mybins$breaks
# # par(new=TRUE)
# # hist(rndat,axes = F, ylab = "", xlab = "", main = "", col = NA, border = NA)
# for (i in 1:c(length(brn)-1)) {
#   x = mean(brn[i],brn[i+1])
#   y = crn[i]
#   points(x,y, col = scales::alpha("black",.2), pch = 19)
# }

# text(x = 2.2, y = .38,labels = paste0("qnorm(1-p)"),adj = 0,pos = 1,offset = 0)
# arrows(x0 = 2.2,x1 = qnorm(1-p), y0 = .35,y1 = .27,code = 2,length=.1)


```
]



---
## Quantile value for a normal distribution 

If X is a random variable distributed with a *Standard normal distribution*, what is the probability of finding X less or equal to $1.645$ or $X\sim N(0,1)$ where $P(X\leq 1.645)$? 
.pull-left[
```{r pnorm_qnorm2}
pnorm(1.645)

qnorm(p = 0.05,lower.tail = F)
qnorm(p = 0.025,lower.tail = F)
```
]

--

.pull-right[
```{r normal_shade_95_5, echo=FALSE, fig.width=5,fig.height=5}
x <- seq(-5, 5, 0.1)
cex  = 1
plot(x, dnorm(x, 0, 1), main = "Density normal", type = "l", lwd = 3, col = "black", ylab = "", xlab = "x", ylim = c(0,1))
quantile.normal = qnorm(c(.95, .05))
abline(v = quantile.normal[!is.infinite(quantile.normal)], lty = 3, lwd = .3) 
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

p = 0.05

# add the polygon to the right 
lb <- qnorm(1-p) # Lower bound
ub <- max(x)   # Upper bound
x2 <- seq(lb, max(x), length = 100) # New Grid
y <- dnorm(x2, 0, 1) # Density
polygon(c(lb, x2, ub), c(0,y,0), col = rgb(1, 0, 0, alpha = 0.5))
text(x = lb, y = p+0.04,labels = paste0(p*100,"%"),adj = 0,pos = 4, cex=cex)

# Add the middle (blue) polygon 
lb <- min(x) # Lower bound
ub <- qnorm(1-p)   # Upper bound
x2 <- seq(lb, ub, length = 100) # New Grid
y <- dnorm(x2, 0, 1) # Density
polygon(c(lb, x2, ub), c(0,y,0), col = rgb(0, 0, 1, alpha = 0.5))
text(x = 0, y = .2,labels = paste0((1-p)*100,"%"),adj = 0,pos = 1, cex=cex)

text(x = 0, y = .55,labels = paste0(quote("Mean")),adj = 0,pos = 1,offset = 0, cex=cex)
arrows(x0 = 0,x1 = 0, y0 = .5,y1 = .42,code = 2,length=.1)
```
]


---
## Normal distribution 

```{r normal_area_function, echo=FALSE, eval=TRUE}
draw.normal <- function(mean = 0, sd = 1, set.seed=1, prob = 0.025, text = FALSE, text.height = .55, where = c("both","left","right")) {
  set.seed(set.seed)
  x <- seq(-5, 5, 0.1)
  cex = 1
  plot(x, dnorm(x, mean, sd), 
       # main = "Density normal", 
       main = "", 
       type = "l", lwd = 3, col = "black", ylab = "", xlab = "x", ylim = c(0,1))
  # abline(v = quantile.normal[!is.infinite(quantile.normal)], lty = 3, lwd = 1, col = c("black")) 
  
  # Horizontal 
  abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)
  
  p = prob 
  
  if(where=="both"){
  # add the polygon to the left  
  lb <- min(x) # Lower bound
  ub <- qnorm(p)   # Upper bound
  x2 <- seq(min(x), ub, length = 100) # New Grid
  y <- dnorm(x2, 0, 1) # Densitypolygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
  polygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
  text(x = -2, y = .2,
       labels = paste0(p*100,"%"),adj = 0,pos = 2, cex=cex)
  
  if (text) {
    text(x = ub, y = .2,
         labels = paste0(round(qnorm(p),2)),adj = 0,pos = 1,offset = 0, cex=cex)
    arrows(x0 = ub,x1 = ub, y0 = .2,y1 = .1,code = 2,length=.1)
  }
  
  # add the polygon to the right 
  lb <- qnorm(1-p) # Lower bound
  ub <- max(x)   # Upper bound
  x2 <- seq(lb, max(x), length = 100) # New Grid
  y <- dnorm(x2, 0, 1) # Density
  polygon(c(lb, x2, ub), c(0,y,0), col = rgb(0, 0, 1, alpha = 0.5))
  text(x = 2, y = .2,
       labels = paste0(p*100,"%"),adj = 0,pos = 4, cex=cex)
  
  if (text) {
    text(x = lb, y = .2,
         labels = paste0(round(qnorm(1-p),2)),adj = 0,pos = 1,offset = 0, cex=cex)
    arrows(x0 = lb,x1 = lb, y0 = .2,y1 = .1,code = 2,length=.1)
  }
  
  # Add the middle (red) polygon 
  lb <- qnorm(p) # Lower bound
  ub <- qnorm(1-p)   # Upper bound
  x2 <- seq(lb, ub, length = 100) # New Grid
  y <- dnorm(x2, 0, 1) # Density
  polygon(c(lb, x2, ub), c(0,y,0), col = rgb(1, 0, 0, alpha = 0.5))
  text(x = mean(x2), y = text.height,
       labels = paste0((1-2*p)*100,"%"),adj = 0,pos = 1, cex=cex)
  
  if (text) {
    text(x = 0, y = .55,
         labels = paste0(quote("Mean")),adj = 0,pos = 1,offset = 0, cex=cex)
    arrows(x0 = 0,x1 = 0, y0 = .5,y1 = .42,code = 2,length=.1)
  }
}
  
  if (where=="left") {
    # add the polygon to the left  
    lb <- min(x) # Lower bound
    ub <- qnorm(p)   # Upper bound
    x2 <- seq(min(x), ub, length = 100) # New Grid
    y <- dnorm(x2, 0, 1) # Densitypolygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
    polygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
    text(x = -2, y = .2,
         labels = paste0(p*100,"%"),adj = 0,pos = 2, cex=cex)
    
    if (text) {
      text(x = ub, y = .2,
           labels = paste0(round(qnorm(p),2)),adj = 0,pos = 1,offset = 0, cex=cex)
      arrows(x0 = ub,x1 = ub, y0 = .2,y1 = .1,code = 2,length=.1)
    }
    
    # Add the middle (red) polygon 
    lb <- qnorm(p) # Lower bound
    ub <- max(x)   # Upper bound
    x2 <- seq(lb, ub, length = 100) # New Grid
    y <- dnorm(x2, 0, 1) # Density
    polygon(c(lb, x2, ub), c(0,y,0), col = rgb(1, 0, 0, alpha = 0.5))
    text(x = 2, y = .2,
         labels = paste0((1-p)*100,"%"),adj = 0,pos = 4, cex=cex)
    
    if (text) {
      text(x = 0, y = .55,
           labels = paste0(quote("Mean")),adj = 0,pos = 1,offset = 0, cex=cex)
      arrows(x0 = 0,x1 = 0, y0 = .5,y1 = .42,code = 2,length=.1)
    } 
    
  }
  
  if (where=="right"){
    # add the polygon to the left  
    lb <- min(x) # Lower bound
    ub <- qnorm(1-p)   # Upper bound
    x2 <- seq(min(x), ub, length = 100) # New Grid
    y <- dnorm(x2, 0, 1) # Densitypolygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
    polygon(c(lb, x2, ub), c(0, y, 0), col = rgb(1, 0, 0, alpha = 0.5))
    text(x = -2, y = .2,
         labels = paste0((1-p)*100,"%"),adj = 0,pos = 2, cex=cex)
    
    if (text) {
      text(x = ub, y = .2,
           labels = paste0(round(qnorm(p),2)),adj = 0,pos = 1,offset = 0, cex=cex)
      arrows(x0 = ub,x1 = ub, y0 = .2,y1 = .1,code = 2,length=.1)
    }
    
    # Add the middle (red) polygon 
    lb <- qnorm(1-p) # Lower bound
    ub <- max(x)   # Upper bound
    x2 <- seq(lb, ub, length = 100) # New Grid
    y <- dnorm(x2, 0, 1) # Density
    polygon(c(lb, x2, ub), c(0,y,0), col = rgb(0, 0, 1, alpha = 0.5))
    text(x = 2, y = .2,
         labels = paste0((p)*100,"%"),adj = 0,pos = 4, cex=cex)
    
    if (text) {
      text(x = 0, y = .55,
           labels = paste0(quote("Mean")),adj = 0,pos = 1,offset = 0, cex=cex)
      arrows(x0 = 0,x1 = 0, y0 = .5,y1 = .42,code = 2,length=.1)
    }
    
  }  
}
```

.tiny[
```{r normal_dist_area, fig.width=9,fig.height=6}
par(mfrow=c(2,3), mar = c(4,4,1,1), cex = 1.1)
# The function is not shown, but can be found in the script (Markdown)
draw.normal(where = "both",  prob = 0.05/2)
draw.normal(where = "both",  prob = 0.2/2 )
draw.normal(where = "both",  prob = 0.5/2 )
draw.normal(where = "both",  prob = 0.95/2)
draw.normal(where = "left",  prob = 0.05  )
draw.normal(where = "right", prob = 0.05  )
```
]

---
## Normal distribution 

Find the probability of finding data relative to a standard deviation number 

```{r Normal_pdf_important_values}
sd = 1
probability.left.side = (pnorm(q = -c(sd*1,sd*2,sd*3),lower.tail = T)*100)
probability.right.side = (pnorm(q =  c(sd*1,sd*2,sd*3),lower.tail = T)*100)
percent.data.under.curve = probability.right.side - probability.left.side
round(percent.data.under.curve,2)

qnorm(p = c(.75,.975, .995),lower.tail = T)
```


---
## Convert a Normal dist. to Standard normal 

- If your data is normally distributed (i.e., $X \sim N(\mu, \sigma)$), you can convert the data to be $X \sim N(0, 1)$ with a scaling
$$Z = \frac{(X-\mu)}{\sigma} $$ 

```{r Standard_normal_transformation, echo=FALSE, fig.width=10,fig.height=5}
par(mfrow = c(1,2))
set.seed(1235)
x <- seq(-5, 20, 0.1)
cex  = 1
black.5 = scales::alpha("black",.5)
red.5 = scales::alpha("red",.5)
blue.5 = scales::alpha("blue",.5)
# Add the standard normal 
plot(x, dnorm(x, 0, 1), main = "Density normal", type = "l", lwd = 3, col = black.5, ylab = "", xlab = "x", ylim = c(0,1))
text(x = 0, y = .62,labels = paste0("Standard \nnormal"),adj = 0,pos = 1,offset = 0, cex=cex)
arrows(x0 = 0,x1 = 0, y0 = .5,y1 = .42,code = 2,length=.1)

# Add a population distribution 
lines(x, dnorm(x, 15, 2), main = "Density normal", type = "l", lwd = 3, col = red.5, ylab = "", xlab = "x", ylim = c(0,1))
text(x = 15, y = .55,labels = paste0("Normal \n mean=15 \n sd=2"),adj = 0,pos = 1,offset = 0, cex=cex)
arrows(x0 = 15,x1 = 15, y0 = .35,y1 = .25,code = 2,length=.1)

# Add simulated data 
my.data = rnorm(100, 15, 2)
mean.data = mean(my.data)
sd.data = sd(my.data)
lines(density(my.data), col = blue.5, lwd = 3)
# Add guides
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

# Add the standard normal 
plot(x, dnorm(x, 0, 1), main = "Density normal", type = "l", lwd = 3, col = black.5, ylab = "", xlab = "x", ylim = c(0,1))

# Add a population distribution 
lines(x, dnorm(x, 0, 1), main = "Density normal", type = "l", lwd = 3, col = red.5, ylab = "", xlab = "x", ylim = c(0,1))
lines(density((my.data-mean.data)/sd.data), col = blue.5, lwd = 3)

# Add guides
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

```




---
## Challenge 3 ![:cube]()

Let's say you want to simulate the length of different beak sizes for 10 birds. 

Use `rnorm()` to generate 10 random normal numbers. 

```{r rnom_function, eval=FALSE}
rnorm()
```


---
## Challenge 3 - Solution ![:cube]()

Let's say you want to simulate the length of different beak sizes for 10 birds. 

Use `rnorm()` to generate 10 random normal numbers. 

```{r rnom_function_example}
set.seed(1234)
n <-10
rnorm(n)
```

- Why are there negative values? Because the mean = 0, so some values spill over the mean on both sides, and in the negative numbers. 


###############
---
class: inverse, center, middle

# Simulating data for power analysis

---
## Power analysis

For the moment, please refer to [this link](https://www.r-bloggers.com/2020/05/power-analysis-by-data-simulation-in-r-part-ii/).

.tiny[
```{r feel_the_power, fig.width=3,fig.height=3}
set.seed(1234)
n = 30
group1 <- rnorm(n = n, mean = 1, sd = 2)
group2 <- rnorm(n = n, mean = 0, sd = 2)
hist(group1, breaks = 10, main = "Histogram of both groups", xlab = "")
hist(group2, add = TRUE, breaks = 10, col= scales::alpha("blue",.5))
t.test(group1, group2, paired = FALSE, var.equal = TRUE, conf.level = 0.9)

set.seed(1)
n_sims <- 10 # we want 1000 simulations
p_vals <- c()
power_at_n <- c(0) # this vector will contain the power for each sample-size (it needs the initial 0 for the while-loop to work)
cohens_ds <- c()
cohens_ds_at_n <- c() 
n <- 30 # sample-size 
i <- 2
while(power_at_n[i-1] < .95){
  for(sim in 1:n_sims){
    group1 <- rnorm(n,1,2) # simulate group 1
    group2 <- rnorm(n,0,2) # simulate group 2
    p_vals[sim] <- t.test(group1, group2, paired = FALSE, var.equal = TRUE, conf.level = 0.9)$p.value # run t-test and extract the p-value
    cohens_ds[sim] <- abs((mean(group1)-mean(group2))/(sqrt((sd(group1)^2+sd(group2)^2)/2))) # we also save the cohens ds that we observed in each simulation
  }
  power_at_n[i] <- mean(p_vals < .10) # check power (i.e. proportion of p-values that are smaller than alpha-level of .10)
  cohens_ds_at_n[i] <- mean(cohens_ds) # calculate means of cohens ds for each sample-size
  n <- n+1 # increase sample-size by 1
  i <- i+1 # increase index of the while-loop by 1 to save power and cohens d to vector
}
power_at_n <- power_at_n[-1] # delete first 0 from the vector
cohens_ds_at_n <- cohens_ds_at_n[-1] # delete first NA from the vector
plot(30:(n-1), power_at_n, xlab = "Number of participants per group", ylab = "Power", ylim = c(0,1), axes = TRUE)
abline(h = .95, col = "red")

plot(30:(n-1), cohens_ds_at_n, xlab = "Number of participants per group", ylab = "Cohens D", ylim = c(0.45,0.55), axes = TRUE)
abline(h = .50, col = "red")
```
]


###############
---
class: inverse, center, middle

# Simulating data for models

---
## Linear model (lm) refresher

$$Y = \beta_{0} + \beta_{1} x_{1} + \cdots + \beta_{p} x_{p} + \epsilon$$

- $Y$ is the response variable
- $\beta_0$ is the intercept
- $\beta_1$ is the coefficient of variation for the first explanatory variable ($x_1$)
- $\beta_p$ is the coefficient of variation for the $p^{th}$ explanatory variable for the $p^{th}$ $x_p$ explanatory variable
- $\epsilon$ is the residual of the model. Note that $\epsilon \sim N(\mu=0,sd = \sigma)$
- The goal is to find the **best estimation** of the parameters ($\beta$s), while minimizing the residuals, and assess the goodness of fit of the model

Reference for the section 
- [Linear model](https://aosmith.rbind.io/2018/01/09/simulate-simulate-part1/) and [Linear mixed model](https://aosmith.rbind.io/2018/04/23/simulate-simulate-part-2/)
- [Poisson model](https://aosmith.rbind.io/2018/07/18/simulate-poisson-edition/)
- [binomial generalized linear mixed model](https://aosmith.rbind.io/2020/08/20/simulate-binomial-glmm/)

---
## Generalized linear model (GLM) refresher

| Type | Equation |
| ---- | -------- |
| Linear | $Y = \beta_{0} + \beta_{1} x_{1}  + \epsilon$ |
| Poisson | $Y \sim Poisson(\mu)$ with $\text {ln} \mu=\beta_0+\beta_1x$ or $\mu=e^{\beta_0+\beta_1x}$ (no separate error term as $\lambda$ determines both the mean and variance) |
| Logistic | $Y \sim Binomial(p)$ with $\text{log} \Bigl(\frac{p}{1-p}\Bigr) = \beta_0 +\beta_1x$ or $p=\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$ where $\text{log} \Bigl(\frac{p}{1-p}\Bigr)$ is the log odds or log likelihood. The Y values are determined by a Bernoulli distribution (binomial of size = 1) |

<!-- 
In GLM, we don'T model the individual Y values, but the mean
https://www.theanalysisfactor.com/generalized-linear-models-no-error-term/
In other words "there's no common error distribution independent of predictor values"
see https://stats.stackexchange.com/questions/124818/logistic-regression-error-term-and-its-distribution
-->
<!-- For poisson (there is no 'error' term like linear regression, since there is inherent variability)?? -->
<!-- logistic regression there is an error (or randomness hidden in the Bernoulli (or binomial(size =1)) with probability p)  -->
<!-- Simulate logistic regression https://stats.stackexchange.com/questions/46523/how-to-simulate-artificial-data-for-logistic-regression/46525 -->

---
## GLM: Logistic

- Recall that logistic is $Y \sim Binomial(p)$ with $\text{log} \Bigl(\frac{p}{1-p}\Bigr) = \beta_0 +\beta_1x$ or $p=\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$ where $\text{log} \Bigl(\frac{p}{1-p}\Bigr)$ is the log odds or log likelihood. The Y values are determined by a Bernoulli distribution (binomial of size = 1) 

<!-- Inspiration : https://stats.stackexchange.com/questions/46523/how-to-simulate-artificial-data-for-logistic-regression/46525
-->
```{r sim_logistic}
set.seed(987654)
n = 1000
x1 = rnorm(n = n, mean = 6, sd = 1)
x2 = rnorm(n = n, mean = 0, sd = 1)
# Rescale the data
x1z = scale(x1)
x2z = scale(x2)
z = 1 + 2*x1z + 3*x2
pr = 1/(1+exp(-z)) # inverse-logit function; Note that 1/(1+exp(-x))== exp(x)/(1+exp(x)), same as pr2 = boot::inv.logit(z)
y = rbinom(n = n, size = 1, prob = pr) # Bernoulli response variable (which is a special case of the binomial with size =1 )

# Combine the data in a dataframe 
df = data.frame(y = y, x1 = x1, x2 = x2)
```

---
## GLM: Logistic

```{r sim_logistic_glm, fig.width=5,fig.height=5}
#now feed it to glm:
glm.logist = glm( y~x1+x2, data=df, family="binomial")
plot(y~x1, data = df, col = scales::alpha("black",.5), pch = 19)
newdata <- data.frame(x1=seq(min(x1), max(x1),len=n), x2 = seq(min(x2), max(x2),len=n))
newdata$y = predict(object = glm.logist, newdata = newdata, type = "response") 
lines(x = newdata$x1,
      y = newdata$y, col = "red",lwd = 2)
```

---
## GLM: Logistic

.tiny[
```{r sim_logistic_glm_3D}
# scatterplot3d::scatterplot3d(x = x1,y = x2,z = y)
library(plotly)
fig <- plot_ly(df, x = ~x1, y = ~x2, z = ~y,
               marker = list(color = ~y, colorscale = c('#FFE1A1', '#683531'), showscale = TRUE))
fig <- fig %>%add_markers() %>% layout(scene = list(xaxis = list(title = 'x1'),
                                   yaxis = list(title = 'x2'),
                                   zaxis = list(title = 'y')),
                      annotations = list(x = 1.08,y = 1.05,
                        text = 'Scale', xref = 'paper',yref = 'paper',
                        showarrow = FALSE)) 
fig
```
]

---
## GLM: Poisson

- Recall that Poisson is $Y \sim Poisson(\mu)$ with $\text {ln} \mu=\beta_0+\beta_1x$ or $\mu=e^{\beta_0+\beta_1x}$ (no separate error term as $\lambda$ determines both the mean and variance)

```{r sim_poisson_glm}
set.seed(42)
n = 1000
x = rnorm(n = n, mean = 0, sd = 1)
# Rescale the data
xz = scale(x)
log.mu = 1 + 2*xz
y = rpois(n = n, lambda = exp(log.mu)) 

# Combine the data in a dataframe 
df = data.frame(y = y, x = x)
```

---
## GLM: Poisson
```{r sim_poisson_glm_plot, fig.width=5,fig.height=5}
#now feed it to glm:
glm.poisson = glm( y~x, data=df, family="poisson")
plot(y~x, data = df, col = scales::alpha("black",.5), pch = 19)
newdata <- data.frame(x=seq(min(x), max(x),len=n))
newdata$y = predict(object = glm.poisson, newdata = newdata, type = "response") 
lines(x = newdata$x,
      y = newdata$y, col = "red",lwd = 2)
```



---
## Simulate categories (t-test, Anova)
<!-- Although this is with only 2 groups, I'm showing this as a quick example -->
.pull-left[
```{r Sim_t_test_anova}
set.seed(1234); n = 1000
y1 = rnorm(n, mean = 15, sd = 1)
y2 = rnorm(n, mean = 15.5, sd = 1)

sim.aov1 = data.frame(y = y1, gr = "A")
sim.aov2 = data.frame(y = y2, gr = "B")
df.aov = rbind(sim.aov1, sim.aov2)
df.aov$gr = factor(df.aov$gr)

# t.test(y~gr, data = df.aov) or
aov.out = aov(y~gr, data = df.aov)
#summary(aov.out)
tk.test = TukeyHSD(aov.out)
round(tk.test$gr,2)
```
]

.pull-right[
```{r Sim_t_test_anova_plot, fig.width=5,fig.height=5}
plot(y~gr, data = df.aov)
```

]

---
## Linear mixed model (LMMs) refresher

- A simple linear model is actually the 'simplest' mixed model (although the convention is that we don't called it mixed)
- What is the random effect in a linear model? 

$$Y = \beta_{0} + \beta_{1} x_{1} + \cdots + \beta_{p} x_{p} + \epsilon$$

---
## Linear mixed model (LMMs) refresher

- A simple linear model is actually the 'simplest' mixed model (although the convention is that we don't called it mixed)
- What is the random effect in a linear model? 

$$Y = \beta_{0} + \beta_{1} x_{1} + \cdots + \beta_{p} x_{p} + \epsilon$$

- The residual of the model ( $\epsilon$ ) is actually the 'random effect' since it is drawn from a distribution which can change for each data point $\epsilon \sim N(\mu=0,sd = \sigma)$. 

---
## Linear mixed model (LMMs) refresher

- to simplify the linear model equation we are going to write it this way:
$$\mathbf{Y} = \mathbf{X}\beta + \epsilon$$
- This only means that $\mathbf{Y}$ is a vector (or matrix) made up of the linear combination of fixed effects $\mathbf{X}\beta$ and a random part $\epsilon$. 

- In this type of notation, LMMs is (random intercept):
$$\mathbf{Y} = \mathbf{X}\beta + \mathbf{Z\upsilon} + \epsilon$$

- With all the previous elements being the same but the random part which is composed of $\mathbf{Z \upsilon} + \epsilon$, for all categories $\mathbf{\upsilon}$. $\mathbf{X}$ is the *design matrix* and $\mathbf{Z}$  is the *block matrix*.
- Just keep in mind that, as before, $\epsilon \sim N(\mu=0,sd = \sigma)$ and $\mathbf{\upsilon} \sim N(\mu=0,sd = \mathbf{D})$, where $\mathbf{D}$ is a variance-covariance matrix. Also, $\mathbf{\upsilon}$ and $\epsilon$ are independent. 

<!-- 
See 
https://towardsdatascience.com/linear-mixed-model-from-scratch-f29b2e45f0a4 
https://www.bristol.ac.uk/cmm/learning/videos/random-slopes.html
-->

---
## Linear mixed model (LMMs) refresher

- This is just another way to write the model (there is a part that specify the random slopes)

$$y_{ij} = \beta_{0} + \beta_{1} x_{1ij} + \upsilon_{1j} x_{1ij} + \upsilon_{0ij} + \epsilon_{0ij}$$
- $\epsilon_{0ij} \sim N(\mu=0,sd = \sigma_{e0})$ and $\mathbf{\upsilon_{0j}} \text{ and } \mathbf{\upsilon_{1j}} \sim N(\mu=0,sd = \Omega_{\upsilon})$ and $\Omega_{\upsilon} = \left[\begin{align*} \sigma_{\upsilon0}^2 & \sigma_{\upsilon10} \\ \sigma_{\upsilon01} & \sigma_{\upsilon1}^2 \end{align*}\right]$


---
## Linear mixed model (LMMs) refresher

- Just so that everything is extra clear
$$\mathbf{Y} = \mathbf{X}\beta + \mathbf{Z \upsilon} + \epsilon$$

One implementation of that could be ($\beta_1$ and $\beta_2$ could represent different treatments):

$$
\left[\begin{align}{l}
y_{11} \\
y_{21} \\
y_{12} \\
y_{22}
\end{array}\right] =
\left[\begin{array}{ll}
1 & 0 \\
0 & 1 \\
1 & 0 \\
0 & 1
\end{array}\right]
\left[\begin{array}{l}
\beta_{1} \\
\beta_{2}
\end{array}\right] + 
\left[\begin{array}{ll}
1 & 0 \\
1 & 0 \\
0 & 1 \\
0 & 1
\end{array}\right]
\left[\begin{array}{l}
\upsilon_{1} \\
\upsilon_{2}
\end{array}\right] + 
\left[\begin{array}{l}
\epsilon_{11} \\
\epsilon_{21} \\
\epsilon_{12} \\
\epsilon_{22}
\end{array}\right]
$$


---
## LMMs simulation

- (G)LMMs are a neat extention of the LM models that can take into account (see [QCBS workshop ](http://r.qcbs.ca/Workshops/workshop07/workshop07-en/workshop07-en.html#1) on the topic)

For now, please see [this link](https://debruine.github.io/tutorials/sim-lmer.html). 

```{r lmmmmmmmmms}
library(lmerTest)
n = 20
sd.n = 2
# Generate dataframe 
x = 1:n
values = rnorm(n = n,mean = 0,sd = sd.n)
gr = rep(c("short","tall"), each = n/2)
sim.df = data.frame(x,values,gr)

plot(density(sim.df[sim.df$gr%in%"short","values"]), col = "black", ylim = c(0,1), main = "Density")
lines(density(sim.df[sim.df$gr%in%"tall","values"]), col = "red")
legend("toprigh",legend = c("Short","Tall"),col = c("black","red"), lty = 1)
```


###############
---
class: inverse, center, middle

# Advanced simulations



---
## Advanced simulations

- Now that we have the building blocks to make simulations, we are going to add another layer:
- We want to simulate a process and then see how our simulation responds to a fluctuation in a parameter
- For this, we need a certain workflow that resembles this: 
  1. create a blank vector in which a value of interest will be stored in 
  2. Within a loop or many `for(){}` loops, we are going to simulate data, get a desired result and store the result for future analysis 
  3. Summarize our findings (in a plot, with calculations, etc.) 

.alert[ATTENTION]. This is an iterative process. Usually, we make small increments writing up the code and then put it all together in a cohesive function that will make exactly what we are looking for. 


---
## Advanced simulations
.tiny[
```{r simulate_sampling_function}
# Defining the population 
n = 600 # Number of elements to be generated 
set.seed(13) # Set RNG 
x = rnorm(n) # Generate numbers from normal distribution 
reedddd = scales::alpha("blue",.4) # Make colour transparent 

# Definte the function 
sample.mean.pop.est <- function(x,n, sample.size, ylim = NULL) {
  # x: is the actual values of the trait measured 
  # n: size of the population (number of individuals or items)
  # sample.size: how big is the sample size from which the MEAN will be calculated from 
  # ylim: add a maximum if needed 
  # histogram of the population 
  
  # Just get the stats from the histogram 
  pop.hist = hist(x, plot = F) # Make base histogram 
  
  # Make empty vector
  tmp.v = c(NA) 
  
  # For loop to calculate the mean based on a sample from the population 
  for (i in 1:n) {
    tmp = sample(x = x, size = sample.size, replace = FALSE)
    # Record that information (mean of the sample)
    tmp.v[i] = mean(tmp)
  } # End i
  
  # Sample histogram 
  sample.hist = hist(tmp.v, plot = F)
  # Population histogram 
  hist(x, ylim = range(c(0,c(sample.hist$counts, pop.hist$counts), ylim)), 
       main = paste("Sample n =", sample.size))
  # Add the sample estimate 
  sample.hist = hist(tmp.v, col = reedddd, add=T)
} # End sample.mean.pop.est
```
]

---
## Advanced simulations

.small[
```{r simulate_sampling_plots}
par(mfrow=c(2,2), lwd = .3)
sample.mean.pop.est(x = x, n = n, sample.size = 1, ylim = 300)
sample.mean.pop.est(x = x, n = n, sample.size = 10, ylim = 300)
sample.mean.pop.est(x = x, n = n, sample.size = 50, ylim = 300)
sample.mean.pop.est(x = x, n = n, sample.size = 500, ylim = 300)
```
]

---
## References
.tiny[
Book
- Otto, S.P. & Day, T., 2007. A Biologist's Guide to Mathematical Modeling in Ecology and Evolution. pp.1‚Äì973.

<!-- ![:scale 5%](images/Otto_Day.2007. Biologist Guide to Mathematical Modeling in Ecology and Evolution.jpg) -->

Links 
- [Getting started simulating data in R: some helpful functions and how to use them (Ariel Muldoon)](https://aosmith.rbind.io/2018/08/29/getting-started-simulating-data/)
- [Simulate Linear model](https://aosmith.rbind.io/2018/01/09/simulate-simulate-part1/) and [Simulate Linear mixed model](https://aosmith.rbind.io/2018/04/23/simulate-simulate-part-2/) by Ariel Muldoon
- [Simulate Poisson model (Ariel Muldoon)](https://aosmith.rbind.io/2018/07/18/simulate-poisson-edition/)
- [Simulate binomial generalized linear mixed model (Ariel Muldoon)](https://aosmith.rbind.io/2020/08/20/simulate-binomial-glmm/)
- [Statistical Simulation in R with Code ‚Äî Part 1](https://towardsdatascience.com/statistical-simulation-in-r-part-1-d9cb4dc393c9)
- [Power Analysis by Data Simulation in R ‚Äì Part 2](https://www.r-bloggers.com/2020/05/power-analysis-by-data-simulation-in-r-part-ii/)
- [bios221; Lab 3: Simulations in R](https://web.stanford.edu/class/bios221/labs/simulation/Lab_3_simulation.html)
- [R Programming for Data Science; See 20 Simulation](https://bookdown.org/rdpeng/rprogdatascience/simulation.html)
- [Simulation in R (YouTube)](https://www.youtube.com/watch?v=tvv4IA8PEzw)
- [Introduction to Simulations in R](http://www.columbia.edu/~cjd11/charles_dimaggio/DIRE/resources/R/simRreg.pdf)
- [Crump Lab; see Chapter 5 Simulating and Analyzing Data in R](https://www.crumplab.com/programmingforpsych/simulating-and-analyzing-data-in-r.html)
- [Intro to R Lecture Notes; see Chapter 13 Simulations](https://users.phhp.ufl.edu/rlp176/Courses/PHC6089/R_notes/simulations.html)
- [Simulation Cheat Sheet, by Nick Huntington-Klein](https://nickch-k.github.io/introcausality/Cheat%20Sheets/Simulation_Cheat_Sheet.pdf)
- [Simulating mixed effets; see chapter 4](https://debruine.github.io/tutorials/sim-lmer.html)
- [Cours 'Econ 224'; Lab #9 - Logistic Regression Part I](https://ditraglia.com/econ224/lab09.pdf)
- [ECON 41 Labs; useful ressources about distributions](https://bookdown.org/gabriel_butler/ECON41Labs/)
- [Binomial regression in R](https://kkorthauer.org/fungeno2019/methylation/vignettes/1-binomial-regression.html#4_pitfalls_of_glm)
- [Simple Data Simulations in R, of course](https://it.unt.edu/simple-data-simulations)
- [Probability Cheat sheet](https://static1.squarespace.com/static/54bf3241e4b0f0d81bf7ff36/t/55e9494fe4b011aed10e48e5/1441352015658/probability_cheatsheet.pdf)
]

---
class: inverse, center, bottom

# Thank you for attending this workshop!

![:scale 50%](images/qcbs_logo.png)


