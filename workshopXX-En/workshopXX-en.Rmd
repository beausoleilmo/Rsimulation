---
title: "Workshop 11: Simulations in R"
subtitle: "QCBS R Workshop Series"
author: "Qu√©bec Centre for Biodiversity Science"
output:
  xaringan::moon_reader:
    includes:
      in_header: qcbsR-header.html
    lib_dir: assets
    seal: true
    css: ["default", "qcbsR.css", "qcbsR-fonts.css"]
    nature:
      beforeInit: "qcbsR-macros.js"
      highlightLines: true
---

<!-- 
Math equations in Markdown 
https://rpruim.github.io/s341/S19/from-class/MathinRmd.html
-->

```{r setup, echo = F}
# in the end, should be about 1500 lines
knitr::opts_chunk$set(
  comment = "#",
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width=6, fig.height=6,
  fig.retina = 3,
  fig.align = 'center'
)
```

## Outline

1. Why are simulations relevant? 
2. What you need to know before doing simulations? 
3. Let's explore what underlie some statistical processes
3. Simulating data for power analysis (in development)
4. Simulating data for models (lm, lmm, in development)

---
## Prerequisites
<!-- I will be using base plot for this workshop: this is because it is MUCH EASIER to get quick BEAUTIFUL plots and show stuff -->
For this workshop it is useful to have a solid understanding of 

1. Linear models and generalized linear models (GLMs)
2. Programming in R 

See the [QCBS R workshops](https://r.qcbs.ca/) if you want to revise these topics.

- We are going to review some aspects of these workshops, but getting some experience from those subjects can clearly help you for this workshop. 
- We are going to be more explicitly show assumptions of models and use for loops in this workshop 

---
## Learning objectives

1. Develop intuition and skills to perform simulations
2. Explore useful functions when designing simulations 
3. Articulate what are the assumptions underlying a simulation (for the models tested or the scope of the simulation)


###############
---
class: inverse, center, middle

# Why are simulations relevant?


---
## Simulations are games 

- Simulations are games that we make up the rules
- It equips us with a very powerful tool to create 'alternative worlds'. 
- The challenge in this game is to figure out what the parameters are for the processes existing in the natural world. 


.alert[Are these two distribution showing the same random process? ] 
```{r echo=FALSE, fig.width=8, fig.height=4}
par(mfrow=c(1,2))
set.seed(12345)
curve(dnorm(x), from = -5,to = 5, ylim = c(0,1), ylab = "Density", lwd =3)
plot(density(rnorm(6)), ylim = c(0,1),main = "", xlim = c(-5,5), xlab = "x", lwd =3)
```


---
## Find patterns in random processes

- Simulations are useful to test the properties of randomly generated data 
- Since we designed the simulation, we know parameters of the processes that underlie it.
- It is then possible to test various methods to 
1. see if they work and verify their assumptions, 
2. do power analysis, 
3. learn how data is generated
4. etc. 

---
## Find patterns in the natural world

- The idea here is that perhaps we are interested in understanding how a certain process was generated (how $X$ influences $Y$, which could be written as $X \xrightarrow{Affects} Y$ or $Y \sim X$). 
- This is probably the most important points for simulations in biology as we are trying to understand **natural processes** (which are themselves manifestations of **random processes**)
- Therefore, being able to do simulations can be a nice addition to your research toolkit. 


---
## Probability distributions are crucial for simulations

- When performing simulations, once has to keep in mind the
1. **type of distribution** underlying the data of interest
2. **parameters** of the distribution itself (mean, standard deviation, rate, degrees of freedom, etc.)
3. **statistical model** making the relationship between the response variable and the explanatory variable. 
4. ideas or things we *want to show or learn* with the simulation

---
## Tips when performing simulation

.alert[Description section]. Add a 'Description' section to all your simulation scripts to introduce what the script is about. 

- You can add information how to use important arguments or a step-by-step description on how to use the script.
- You can add some references that you used to build your scrip. 

.small[
```r
# Description  ------------------------------------------------------------
#### ### ### ## #### ### ### ## #### ### ### ## 
# TITLE OF THE SCRIPT
# Created by YOUR_NAME
# 
# Why: 
# Requires:
# NOTES: 
# Reference : 
#### ### ### ## #### ### ### ## #### ### ### ## 

# Code here ... 

```
]


---
## Tips when performing simulation

.alert[Comments]. Be extra generous when commenting your code to describe as precisely as possible for the information that is not variable. 


.small[
```{r eval=FALSE}
# Description  ------------------------------------------------------------
## This is the description section as previously presented 

# Libraries ---------------------------------------------------------------
## Here load the libraries used in the script 
library(ggplot2)

# Functions ---------------------------------------------------------------
## Add and describe the functions used in the script 
## Add more information
function.name = function(var){tmp=2+var; return(tmp)}

# Plots -------------------------------------------------------------------
## Plotting the data simulated 
## Add more information
plot(function.name(1:10)+rnorm(10))
```
]

.small[
(We used fewer comments to lighten the presentation, but please add comments.)
]

###############
---
class: inverse, center, middle

# When could a simulation be useful? 

---
## ALL THE TIME!!!! 

Whether you want to 
- test a statistical method or a hypothesis, 
- learn how a mechanism work (i.e., genetic drift, natural selection), 
- play with data before your field work, 
- etc. 


###############
---
class: inverse, center, middle

# What you need to know before doing simulations? 


---
## Functions useful in simulations (RNG)

- When performing simulations, you will have to play with (pseudo-)randomly generated numbers from a random number generator (RNG). 
- This is a challenge if we want to replicate the analysis we are performing. 
- `R` has the function `set.seed()` that help us to play with the RNG.

The example below uses a RNG to extract numerical value between 1 and 10
```{r, echo=FALSE}
set.seed(123)
```

```{r}
runif(n = 1, min = 1, max = 10) # Gives a random number between 1 and 10
runif(n = 1, min = 1, max = 10) # RNG wasn't reset, different answer (see above)
runif(n = 1, min = 1, max = 10) # Different again... 

set.seed(42); runif(n = 1, min = 1, max = 10) # This sets the RNG 
set.seed(42); runif(n = 1, min = 1, max = 10) # The exact same number 

```

---
## Functions useful in simulations (sample)

- The function `sample()` randomly picks a value from a vector.

The example below uses a RNG to extract numerical value between 1 and 10
```{r, echo=FALSE}
set.seed(123)
```

```{r}
set.seed(12) # Set the RNG 
v.1.10 = 1:10 # Make a vector from 1 to 10 
# Randomly pick 1 (size) value from the vector (x), without replacement 
sample(x = v.1.10, size = 1, replace = FALSE) 
```

- The values don't have to be numerical: they could be characters or factors

```{r}
set.seed(3) # Set the RNG 
# Randomly pick 5 (size) letters from the vector (x), without replacement 
sample(x = LETTERS, size = 5, replace = FALSE) 
sample(x = as.factor(month.abb), size = 5, replace = FALSE) 
```

---
## Functions useful in simulations (sample)
<!-- SEE http://faculty.washington.edu/kenrice/sisg/sisg-lie11-05.pdf -->

- The function `sample()` can actually be used in order to do permutations 
- Let's say we have a data frame with 2 columns 

.small[
.pull-left[
```{r, echo=FALSE}
library(viridis)
```

```{r, fig.width=4,fig.height=3}
set.seed(123)
n = 40; col = viridis::viridis(n = n)
x = 1:n ; y = 2+.5*x + rnorm(n, sd=7)
df.xy = data.frame(x,y, col )
```
]

.pull-right[
```{r, fig.width=4,fig.height=3}
df.xy$x.s = sample(x)
```
We break up the link of X and Y 
]

```{r, fig.width=7,fig.height=3}
par(mfrow=c(1,2), mar=c(4,4,1,1))
plot(y~x,  col=col, data=df.xy, pch=19);abline(lm(y~x,  data=df.xy)) # Original 
plot(y~x.s,col=col, data=df.xy, pch=19);abline(lm(y~x.s,data=df.xy)) # Permutated 
```
]

<!-- --- -->
<!-- ## Functions useful in simulations (sample) -->
<!-- - Keep in mind that permutations are valid if the null hypothesis tested is that there is *no association* between the variables studied.
http://faculty.washington.edu/kenrice/sisg/sisg-lie11-05.pdf
-->

---
## Functions useful in simulations (rep)

- You will not only see numerical values in simulations. Characters (or factors) can be generated in R easily. 
- The `rep()` function can help you with this

```{r}
(let4first = LETTERS[1:4])
rep(let4first, times = 2) # Repeat the WHOLE sequence twice 
rep(let4first, each = 2) # Repeat each element twice 
# Set the number of repeat for each element in the vector 
rep(let4first, times = c(1,2,3,6))
# Complete replication: replicate each element twice and do this three times 
rep(let4first, each = 2, times = 3)
rep(let4first, length.out = 6) # Repeat the vector until you hit the maximum length

```

---
## Functions useful in simulations

- In some cases, you want to replicate a process, for example when you want to generate multiple populations at once with the same parameters
- The `replicate()` can be used instead of a for loop and make simulations faster (I assume). 

```{r data_replicate}
set.seed(1234)
data.replicated = replicate(n = 2,expr = data.frame(gr = rep(LETTERS[1:3],each = 2),
                                                    y = rnorm(6)), 
                            simplify = FALSE)
```
.pull-left[
```{r data_replicate1}
data.replicated[[1]]
```
]
.pull-right[
```{r data_replicate2}
data.replicated[[2]]
```
]

---
## Challenge 1 ![:cube]()

Select randomly 4 values out of a vector of numbers ranging from 1 to 10 : 1. without replacement and 2. with replacement. 

Use `sample()` to perform this task. 

```{r show_sample, eval=FALSE}
sample()
```


---
## Challenge 1 - Solution ![:cube]()

Select randomly 4 values out of a vector of numbers ranging from 1 to 10 : 1. without replacement and 2. with replacement.

```{r sample_replacement}
set.seed(12345) # Sets the random number generator to a fix value
vec.1.10 = 1:10 # Make the vector to choose from 
sample(x = vec.1.10, size = 4, replace = FALSE) # Sample 4 nb without replacement
sample(x = vec.1.10, size = 4, replace = TRUE) # Sample 4 nb with replacement
```

As you can see in the last example, there are 2 "6"s, since each time a random number was picked, all numbers could be randomly chosen. 

1000 draws with replacement from 0 or 1, with equal probability for each. 
```{r sample_example}
set.seed(123); table(sample(x = 0:1, size = 1000, replace = T,prob = c(.5,.5)))
```

---
## Challenge 2 ![:cube]()

Create a data frame with variable x ranging from 1 to 10, $y = 2+ 3 * x$ and a grouping factor (gr) with one group with the smallest values and another group with the largest values. 

---
## Challenge 2 - Solution ![:cube]()
```{r linear_no_error, fig.width=8, fig.height=5}
x = 1:10
y = 2 + 3 * x
gr = rep(letters[1:2],each = 5)
linear.df = data.frame(x,y,gr)
plot(y~x, col = as.factor(gr), data = linear.df, pch = 19)
```


###############
---
class: inverse, center, middle

# Let's explore what underly some statistical processes

---
## Probability and odds 

- When describing the chance of an outcome, you can use a probability, odds, or log-odds
- They are related: odds of an event is $odds = p/(1-p)$  or $odds = \frac{\text{probability of success}}{\text{probability of failure}}$
- Why bother? 
  - probability ranges between 0 and 1, but odds can range from 0 to infinity. Log (natural log, or ln) odds can range from negative infinity to infinity. 
  - This will become handy when building a logistic model as this is the 'logit' transformation. 

<!-- .alert[Careful!] This is not the same as saying $success/(success + failure)$. This last calculation is the probability -->
- Example
  - a horse runs 100 races and wins 20 of them.  What is the *odd* of winning?
```{r}
totl.nb.events = 100
successes = 20
odds.favor = (successes)/(totl.nb.events-successes) # Odds in favor of the event (here 1 in 4)
odds.against = (totl.nb.events-successes)/(successes) # Odds in against of the event (here 4 to 1)

library(MASS)
```
<!-- In the calculation above, I'm NOT using the probability of successes as the division by 100 (to make it a probability) would cancel out and make it harder to get a quick intuition from the toy example -->

So the horse wins 1 race for 4 fails or the odds of winning is `r fractions(odds.favor)`.

Conversely, the horse the odds of failing is `r paste0(odds.against,":1")` (read 4 *to* 1). 
<!-- In other words, the horse is 4 times more likely to fail than to succeed in a race. (???)  -->


---
## Probability and odds 

- Example
  - a horse runs 100 races and wins 20 of them. What is the *probability* of winning?
```{r}
totl.nb.events = 100
successes = 20
probability.favor = (successes)/(totl.nb.events) # probability of the event (here 20%)
probability.favor
fractions(probability.favor)
```
So the horse *wins* `r fractions(probability.favor)` of the times. 

---
## Probability and odds 

```{r odds_prob, fig.width=9,fig.height=7.5}
log_odds = seq(from = -5, to = 5, by = 1)
odds = exp(log_odds)
inv.logit <- function(x) {exp(x)/(1 + exp(x))}
p = inv.logit(log_odds)
p2 = odds/(1 + odds)
# Probability of failure (1-p)
q = 1-p
# store log_odds and y in data frame for use with ggplot
d = data.frame(log_odds, odds, p, p2, q) 
head(d, 4)
```

<!-- Reference 
https://www.montana.edu/rotella/documents/502/Prob_odds_log-odds.pdf 
https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/
https://www.statisticshowto.com/probability-and-statistics/probability-main-index/odds-ratio/
-->
<!-- Good example on odds ration 
  https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_confidence_intervals/BS704_Confidence_Intervals10.html -->
---
## Probability and odds 

.tiny[
```{r odds_prob_plot, fig.width=10,fig.height=5.5}
par(mfrow = c(1,3), mar =c(4,4,1,1), cex = 1.4)
plot(d$odds~d$log_odds, type = "l", ylab = "Odds", xlab = "Log odds", lwd = 3) ; abline(v = 0, lty = 3)
plot(d$p~d$odds, type = "l", ylab = "p", xlab = "Odds", lwd = 3) ; abline(h = .5, v = 0, lty = 3)
plot(d$p~d$log_odds, type = "l", ylab = "p", xlab = "Odds", lwd = 3) ; abline(h = .5, v = 0, lty = 3)
```
]

---
## Why statistical distributions matter? 

- To help us in performing simulations, we need to get some data (random variables, X) which numerical values are outcomes of random processes. 
- The idea here is to find the distribution underlying a random process which generate a particular phenomenon in nature and use it to output values to be used in your simulation.
  - After generating the data, you can analyze it and see how it can be manipulated. 
  

---
## Functions for various distributions

- In `R`, there is a convention on how to call different distributions. 

| Type | Definition | Comment |
| ---- | ---------- | ------- |
| p	   | *probability* (*cumulative* distribution function or CDF; gives you the probability for a given quantile) | gives the probability that a random variable takes in a certain *range of values* for that random variable |
| q	   | *quantile* (inverse of CDF; gives you the x value for a given probability) | gives the position on the x axis where the area under the curve is a certain probability |
| d	   | *density* density function (probability density function or PDF) | gives probability that a random variables have for a specific value if the random variable  |
| r	   | *random* random variable coming from a certain distribution | We use this to generate random numbers from a specific distribution |

- In the next slide, you'll see some built in functions to use these distributions 

---
## Functions for various distributions

.verysmall[
| Distribution | Probability | Quantile | Density | Random | PDF or PMF |
| ------------ | ----------- | -------- | ------- | ------ | ---------- |
| Beta	       | `pbeta`	 | `qbeta`	| `dbeta`	| `rbeta` | |
| Binomial	   | `pbinom`	 | `qbinom`	| `dbinom`	| `rbinom` | $f(x) = {n \choose x} p^x(1-p)^{n-x}$ |
| Cauchy       | `pcauchy` | `qcauchy` | `dcauchy`	| `rcauchy` ||
| Chi-Square	 | `pchisq`	 | `qchisq`	 | `dchisq`	| `rchisq` | |
| Exponential	 | `pexp`    | `qexp`	 | `dexp`	| `rexp` ||
| F	           | `pf`	     | `qf`	| `df`	| `rf` | |
| Gamma	       | `pgamma`	 | `qgamma`	| `dgamma`	| `rgamma` | |
| Geometric	   | `pgeom`	 | `qgeom`	| `dgeom`	| `rgeom` | |
| Hypergeometric | `phyper`	| `qhyper`	| `dhyper`	| `rhyper` ||
| Logistic	     | `plogis`	| `qlogis`	| `dlogis`	| `rlogis` | |
| Log Normal	   | `plnorm`	| `qlnorm`	| `dlnorm`	| `rlnorm` | |
| Negative Binomial	| `pnbinom`	| `qnbinom`	| `dnbinom`	| `rnbinom` | |
| Normal	     | `pnorm`	| `qnorm`	| `dnorm`	| `rnorm` | $f(x)={\frac{1}{\sigma\sqrt{2\pi}}}e^{-{\frac{1}{2}}\Bigl(\frac {x-\mu}{\sigma}\Bigr)^2}$ |
| Poisson	     | `ppois`	| `qpois`	| `dpois`	| `rpois` | $f(x) = e^{-\lambda}\frac{\lambda^x}{x!}$ |
| Student t	   | `pt`	| `qt`	| `dt`	| `rt` ||
| Studentized Range | `ptukey`	| `qtukey`	| `dtukey`	| `rtukey` | |
| Uniform	     | `punif`	| `qunif`	| `dunif`	| `runif` | $f(x) = \frac{1}{b-a}$ for $x \in [a,b]$, $0$ otherwise |
| Weibull	     | `pweibull`	| `qweibull`	| `dweibull`	| `rweibull` ||
| Wilcoxon Rank Sum Statistic	   | `pwilcox`	| `qwilcox`	| `dwilcox`	| `rwilcox` ||
| Wilcoxon Signed Rank Statistic | `psignrank`	| `qsignrank`	| `dsignrank`	| `rsignrank` ||
]

---
## Functions for various distributions

.verysmall[
| Distribution | Probability | Quantile | Density | Random | PDF or PMF |
| ------------ | ----------- | -------- | ------- | ------ | ---------- |
| Beta	       | `pbeta`	 | `qbeta`	| `dbeta`	| `rbeta` ||
| <span style="background-color: #FFFF00">**Binomial**</span>	   | `pbinom`	 | `qbinom`	| `dbinom`	| `rbinom` | $f(x) = {n \choose x} p^x(1-p)^{n-x}$ |
| Cauchy       | `pcauchy` | `qcauchy` | `dcauchy`	| `rcauchy` ||
| <span style="background-color: #FFFF00">**Chi-Square**</span>	 | `pchisq`	 | `qchisq`	 | `dchisq`	| `rchisq` ||
| <span style="background-color: #FFFF00">**Exponential**</span>	 | `pexp`    | `qexp`	 | `dexp`	| `rexp` ||
| <span style="background-color: #FFFF00">**F**</span>	           | `pf`	     | `qf`	| `df`	| `rf` ||
| Gamma	       | `pgamma`	 | `qgamma`	| `dgamma`	| `rgamma` ||
| Geometric	   | `pgeom`	 | `qgeom`	| `dgeom`	| `rgeom` ||
| Hypergeometric | `phyper`	| `qhyper`	| `dhyper`	| `rhyper` ||
| <span style="background-color: #FFFF00">**Logistic**</span>	     | `plogis`	| `qlogis`	| `dlogis`	| `rlogis` ||
| <span style="background-color: #FFFF00">**Log Normal**</span>	   | `plnorm`	| `qlnorm`	| `dlnorm`	| `rlnorm` ||
| Negative Binomial	| `pnbinom`	| `qnbinom`	| `dnbinom`	| `rnbinom` ||
| <span style="background-color: #FFFF00">**Normal**</span>	     | `pnorm`	| `qnorm`	| `dnorm`	| `rnorm` | $f(x)={\frac{1}{\sigma\sqrt{2\pi}}}e^{-{\frac{1}{2}}\Bigl(\frac {x-\mu}{\sigma}\Bigr)^2}$ |
| <span style="background-color: #FFFF00">**Poisson**</span>	     | `ppois`	| `qpois`	| `dpois`	| `rpois` | $f(x) = e^{-\lambda}\frac{\lambda^x}{x!}$ |
| <span style="background-color: #FFFF00">**Student t**</span>	   | `pt`	| `qt`	| `dt`	| `rt` ||
| Studentized Range | `ptukey`	| `qtukey`	| `dtukey`	| `rtukey` ||
| <span style="background-color: #FFFF00">**Uniform**</span>	     | `punif`	| `qunif`	| `dunif`	| `runif` | $f(x) = \frac{1}{b-a}$ for $x \in [a,b]$, $0$ otherwise |
| Weibull	     | `pweibull`	| `qweibull`	| `dweibull`	| `rweibull` ||
| Wilcoxon Rank Sum Statistic	   | `pwilcox`	| `qwilcox`	| `dwilcox`	| `rwilcox` ||
| Wilcoxon Signed Rank Statistic | `psignrank`	| `qsignrank`	| `dsignrank`	| `rsignrank` ||
]

<!-- 
PDF for continuous 
PMF for discrete 
-->

---
## Common statistical distributions
<!-- ![:scale 90%](images/distributions_all.png) -->

```{r, echo=FALSE, out.width=750, out.height=600, fig.width=9,fig.height=7.5}
par(mfrow = c(3,3), bg = NA)
# par(mfrow = c(1,1), bg = NA)
col = scales::alpha("black",.5)
col.r = scales::alpha("red",.5)
col.g = scales::alpha("green",.5)
col.b = scales::alpha("blue",.5)
lwd=2
# Binom
#define range of "successes"
success <- 0:20
plot(success, dbinom(x = success, size=20, prob=0.4),type='h', col = col.b, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Binomial")
curve(expr = dbinom(x = x, size=20, prob=0.6),col = col, lwd=lwd, ylim = c(0,1), type = 'h',add = T)
curve(expr = dbinom(x = x, size=1, prob=0.5),col = col.r, lwd=lwd, ylim = c(0,1), type = 'h',add = T)
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)
legend("topright",legend = c("n = 20, p = .4","n = 20, p = .6","n = 1, p = .5"),lty = c(1,1,1), col =c(col.b,col, col.r), lwd = 2)

# Logistic
curve(dlogis(x,location = 0, scale = 1), from=-10, to=10, col = col, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Logistic, l = 0, s = 1")
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

# Chi-sq
curve(dchisq(x, df = 10), from = 0, to = 40, col = col.b, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Chi-square")
curve(dchisq(x, df = 4), from = 0, to = 40, col = col, lwd=lwd, ylim = c(0,1), add = T)
curve(dchisq(x, df = 1), from = 0, to = 40, col = col.r, lwd=lwd, ylim = c(0,1), add = T)
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)
legend("topright",legend = c("df = 10","df = 4","df = 1"),lty = c(1,1,1), col =c(col.b,col, col.r), lwd = 2)

# Exponential 
curve(dexp(x, rate = .5), from=0, to=10, col=col.b, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Exponential")
curve(dexp(x, rate = .2), from=0, to=10, col=col, lwd=lwd, ylim = c(0,1),add = T)
curve(dexp(x, rate = .8), from=0, to=10, col=col.r, lwd=lwd, ylim = c(0,1),add = T)
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)
legend("topright",legend = c("rate = 0.8","rate = 0.5","rate = 0.2"),lty = c(1,1,1), col =c(col.b,col,col.r), lwd = 2)

# F-distribution
curve(df(x, df1 = 10, df2 = 20), from = 0, to = 4, n = 5000, col= col, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "F-distribution, df1 = 10, df2 = 20")
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

# normal 
curve(expr = dnorm(x = x, mean=0,sd=1), from = -5, to = 5, col=col.b, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Normal")
curve(expr = dnorm(x = x, mean=0,sd=2), from = -5, to = 5, col=col, lwd=lwd, ylim = c(0,1), add = T)
curve(expr = dnorm(x = x, mean=2,sd=1), from = -5, to = 5, col=col.r, lwd=lwd, ylim = c(0,1), add = T)
curve(dt(x, df=1), from=-5, to=5, col=col.g, lty = 1, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "t-distribution df = 10", add=TRUE)
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)
legend("topright",legend = c("Normal, m = 0, sd = 1","Normal, m = 0, sd = 2","Normal, m = 2, sd = 1","t-distribution, df =1"),lty = c(1,1,1,1), col =c(col.b,col,col.r,col.g), lwd = 2)

# Log normal 
curve(dlnorm(x, meanlog=0, sdlog=1), from=0, to=10, col=col, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Log-normal, m = 0 sd = 1")
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

# Poisson
plot(success, dpois(success, lambda=5), type='h', col=col, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Poisson, lambda = 5")
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

# unifrom
curve(dunif(x, min = 8,max = 9), from=5, to=25, col=col.b, lwd=lwd, ylim = c(0,1), ylab = "Density", main = "Uniform") 
curve(dunif(x, min = 10,max = 15), from=5, to=25, col=col, lwd=lwd, ylim = c(0,1), add=T) 
curve(dunif(x, min = 16,max = 18), from=5, to=25, col=col.r, lwd=lwd, ylim = c(0,1), add = T) 
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)
legend("topright",legend = c("min = 8, max = 9","min = 10, max = 15","min = 16, max = 18"),lty = c(1,1,1), col =c(col.b,col,col.r), lwd = 2)
```

---
## Summary of important distribution 

.small[
- Distributions of random variables comes in 2 flavors: 
1. Discrete (only *countable* number of *distinct* values, e.g., 0, 5, 19,...)
2. Continuous (infinite number of possible values, e.g., 0.1,0.62113,...)


| Distribution | Definition | Type of data | Example |
| ------------ | ---------- | ------------ | ------------ |
| Binomial	   | probability of *n* repeated *yes/no* experiments with probability *p* of *success* | discrete; yes/no; 0/1 | Number of heads in a row after 3 tosses |
| Poisson	     | probability of a number of cases happening in a defined range of time or space given a constant rate | discrete; | ticks of a radiation counter, DNA mutation, colonization of animals on remote islands (something per unit time or space for example) |
| Logistic	   | probability of an individual to be in one class or another | continuous; | alive or not, yes or no, diseased or not, etc. |
| Normal	     | probability function which informs how values are distributed | continuous; | height of individuals, beak length, shoe size, time required to run a kilometer, the data shows a 'bell-shaped' curve |
| Uniform	     | probability of all outcomes are equal | continuous; | angle between 0-360 | 
<!-- | Exponential	 | | continuous;| Generating genealofies | -->
<!-- | Student t	   |            | continuous; | | -->
<!-- | Geometric	   |            | continuous; | Gene conversion (recombination) | -->
<!-- | Beta	       |            |              | -->
<!-- | Chi-Square	 |            |              | -->
<!-- | F	           |            |              | -->
]

---
## The same distribution? 

.alert[Careful!]

- Some distribution are similar but don't show the same process 

```{r, echo=FALSE}
par(mfrow = c(1,2))
set.seed(1234)
hist(rbinom(10000, 10, 0.5), #breaks = seq(-0.5, 10.5, by = 1), 
     main = "Binomial")
hist(rnorm(10000, 5, 1.5), xlim = c(0,10), main = "Normal")
```


---
## Other distributions with packages

- Other packages have other distributions. See the [CRAN Task View: Probability Distributions](https://cran.r-project.org/web/views/Distributions.html) if you need a funky distribution that doesn't come in `base R`.
- Some distributions can be recreated in R even if they haven't their own functino name (e.g., Bernoulli). 
  - Bernoulli distribution can be simulated with a binomial distribution with `size = 1`.

```{r, fig.width=5,fig.height=3}
par(mfrow = c(1,2), cex = 1.4)
# install.packages("Rlab")
library("Rlab")

# Bernoulli 
x <- seq(0, 1, by = 1) 
y_dbern <- dbern(x, prob = 0.7)
barplot(height = y_dbern,
        names.arg = setNames(c(.3, .7), c('absent (0)', 'present (1)')), 
        ylim = c(0, 1), xlab = '', ylab = 'probability', main = 'Bernoulli p = 0.7')

# Binomial (to get Bernoulli) 
x <- seq(0, 1, by = 1) 
y_binom <- dbinom(x,size = 1, prob = 0.7)
barplot(height = y_binom,
        names.arg = setNames(c(.3, .7), c('absent (0)', 'present (1)')), 
        ylim = c(0, 1), xlab = '', ylab = 'probability', main = 'Binomial, n = 1, p = 0.7')

# Simulate 10 (fair) coin flips
set.seed(98765)
rbinom(n = 10, size = 1, prob = 0.5)
```

```{r, echo=FALSE, eval=FALSE}
# Coin flips visualize
set.seed(1235)

nb.flips = 10
set.seed(98765)
coin.flips = rbinom(n = nb.flips, size = 1, prob = 0.5)
heads.tails = ifelse(coin.flips==1,"H","T")

# initialize a plot
plot(x = c(0, nb.flips*(radius+.5)), y = c(-.5, 1), type = "n", 
     axes = F,
     asp = 1, 
     ylab = "", xlab = "")
w = 0
pos=0
for (i in 1:nb.flips) {
  if (i %% 5 == 0) {
    pos = pos + 2.5
    w = 0
  }
   
 # prepare "circle data"
  radius = 1
  center_x = w + 1
  center_y = pos
  theta = seq(0, 2 * pi, length = 200) # angles for drawing points around the circle
  
  # draw the circle
  lines(x = radius * cos(theta) + center_x, y = radius * sin(theta) + center_y)
  polygon(x = radius * cos(theta) + center_x, y = radius * sin(theta) + center_y, col = scales::alpha("beige",.5))
  text(center_x,center_y,labels = heads.tails[i], cex = 3)
  w = w + 2*radius+.5
}

```



---
## Poisson distribution

- Poisson distribution assume that 
  - the probability of some event is small over a short period of time (or area), but that there are a lot of events (as there would be a lot of time passing, or a lot of space) in which the process could happen 
  - The number of event reported is independent from one time point to the other. 
  - In summary, it models *rare* events that happen at a certain *rate*. In other words, there are many opportunities to succeed (n is large), but the probability of success (for each trial) is small
  
  $$f(x) = e^{-\lambda}\frac{\lambda^x}{x!}$$ where $\mu = \sigma = \lambda$
  

---
## Poisson distribution vs binomial

- Difference with Binomial :
  - Number of events in Poisson can be infinite (so n tends towards infinity and the probability of each event approaches 0)
  - Number of trials is finite ( $n$ ) in Binomial distribution 
  
```{r, echo=FALSE}
par(mfrow = c(2,2))
x <- 0:10
n <- 10000
barplot(dbinom(x, n, 2/n), names.arg = x, ylim = c(0, 0.35), main = paste("Binomial with n = ", n))
barplot(dbinom(x, n, 9/n), names.arg = x, ylim = c(0, 0.35), main = paste("Binomial with n = ", n))
barplot(dpois(x, 2), names.arg = x, ylim = c(0, 0.35), main = paste("Poisson with Lambda = ", 2))
barplot(dpois(x, 9), names.arg = x, ylim = c(0, 0.35), main = paste("Poisson with Lambda = ", 9))

pbinom(q = 2, size = n, prob = 2/n)
ppois(q = 2, lambda = 2)
```


---
## Poisson distribution

- Examples: 
  - Nb of deaths due to a disease (e.g., SARS-CoV 2) for a period of time 
  - Ticks of a radiation counter (Geiger counter which detects and measures ionizing radiation)
  - DNA mutation
  - Colonization of animals on remote islands (something per unit time or space for example)
  - Mutation models
  - Recombination models 
  - Bacterial colony growing on an agar plate 
  
<!-- Examples taken from
https://ani.stat.fsu.edu/~debdeep/p4_s14.pdf
-->
  
```{r, echo=FALSE, fig.width=5,fig.height=3}
set.seed(1235)
# initialize a plot
plot(c(-1, 3.5), c(-1, 1), type = "n", axes = F, asp = 1, ylab = "", xlab = "")
n = 25
# prepare "circle data"
radius = 1
center_x = 0
center_y = 0
theta = seq(0, 2 * pi, length = 200) # angles for drawing points around the circle

# draw the circle
lines(x = radius * cos(theta) + center_x, y = radius * sin(theta) + center_y)
polygon(x = radius * cos(theta) + center_x, y = radius * sin(theta) + center_y, col = scales::alpha("beige",.5))

center_x = 2.5
center_y = 0
lines(x = radius * cos(theta) + center_x, y = radius * sin(theta) + center_y)
polygon(x = radius * cos(theta) + center_x, y = radius * sin(theta) + center_y, col = scales::alpha("beige",.5))

rdmunif<-runif(n,0,1)
r = runif(n,0,radius) #radius * sqrt(rdmunif)
theta = rdmunif * 2 * pi
x = center_x + sqrt(r*radius) * cos(theta)
y = center_y + sqrt(r*radius) * sin(theta)

points(x,y, pch =19, cex = .6)

arrows(1.1,0,1.4,0, length = .1)
# draw sq
x.pos = c(1.9,2.5, 2.9,2,2.2)
y.pos = c(0,.3,-.6,.3,-.8)
dx = .1
for (i in 1:length(x.pos)) {
  polygon(x = c(x.pos[i],x.pos[i]+dx,x.pos[i]+dx,x.pos[i]),y = c(y.pos[i],y.pos[i],y.pos[i]-dx,y.pos[i]-dx))
}
```

<!-- Useful resource -->
<!-- Poisson definition see https://ani.stat.fsu.edu/~debdeep/p4_s14.pdf -->
<!-- Poisson example see https://www.pnas.org/content/115/37/9270 -->

---
## Binomial distribution

- Binomial distribution assume that 
  - Each *independent* trial is either a success (1, T, Yes) or failure (0, F, No)
    * Note that a 'success' or 'failure' is a *contrast* between 2 elements. 
    * These could be 'heads/tail', 'girl/boy', 'diseased/not', etc.  
  - There is a *fixed* number of trials and the probability on each trial is constant.

<!-- , 'cancer cell death/alive' -->


$$f(x) = {n \choose x} p^x(1-p)^{n-x} $$
.tiny[
where 
- $p$ is the probability of success in a single trial (therefore, $(1-p)$ is the probability of failure, sometimes seen as $q$)
- $n$ is the number of trials 
- $x$ is the number of trials that are successes
- ${n \choose x}$ is the number of combinations (which is equal to $\frac{n!}{r!(n-r)!}$) where the order doesn't matter. 
]

---
## Binomial distribution

- Examples 
  - Number of heads in a row after 3 tosses
  - Genetic drift 
  - Wright-Fisher model 

<!-- Examples taken from 
http://www.biology.arizona.edu/biomath/tutorials/polynomial/applications/BinomialEx3.html
https://www.zoology.ubc.ca/~bio300b/binomialnotes.html
https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/bs704_probability7.html
-->

---
## Binomial distribution

- Examples 
  - probability of having a certain number of offspring given the genotypes of the parents 
    - for example, if a male is AA and female Aa, what is the probability that 6 of their 7 offspring has Aa
      - Find the probability of getting Aa from AA x Aa [1/2 AA and 1/2 Aa], 
      - then out of n=7 offspring from that couple, 6 are of genotype Aa, $P(6|7) = {7 \choose 6} (1/2)^6(1-1/2)^{7-6} = 7/128$)
  - Probability of a certain number of your children being daughters 
    - if you have 5 children, these are independent trials, so n = 5
    - "success" is the probability that the child is a daughter (p = 1/2)
    - $P(3|5) = {5 \choose 3} (1/2)^3(1-1/2)^{5-3} = 5/16$


---
## Normal distribution (Gaussian)

- This distribution is the most important to know (see Central limit theorem). 
- It has interesting properties (mean: center, sd: spread, bell-shaped, symmetry, etc.)
- Can approximate other distribution (might require data transformation)
- The normal distribution with mean = 0 and sd = 1 is called the **Standard Normal**

The general form of the probability density function for a normal distribution is 

$$f(x)={\frac{1}{\sigma\sqrt{2\pi}}}e^{-{\frac{1}{2}}\Bigl(\frac {x-\mu}{\sigma}\Bigr)^2}$$

The important part here is the mean $\mu$ and the standard deviation $\sigma$. Usually summarized as $N(\mu, \sigma)$

<!-- Material inspiration
https://ani.stat.fsu.edu/~debdeep/p4_s14.pdf
-->

---
## Normal distribution (Gaussian)

- Examples: 
  - Distribution of a lot of continuous phenotypes 
    - Arm length, or human height in a population
  - velocity of a collection of molecules in a gas of liquid
  - the error from measurements (length of beak).


---
## Normal distribution (Gaussian)

- The `iris` dataset contains values that are 'drawn' from a normal distribution. 
- Here are the histogram for sepal length (mm).

```{r, echo=FALSE, fig.width=11,fig.height=5}
par(mfrow = c(1,3), cex = 1.4)
spiris = unique(iris$Species)
for (i in 1:length(spiris)) {
  tmp.iris=iris[iris$Species %in% spiris[i],]
  hist(tmp.iris$Sepal.Length,
       main = paste("Iris",spiris[i]),
       xlab = "Sepal length",
       xlim = c(3.8,9), breaks = 10)
}


```


---
## Normal distribution (dissection)

.pull-left[
```{r, fig.width=5,fig.height=5}
curve(expr = dnorm(x = x, mean=0,sd=1), 
      from = -5, to = 5, ylim = c(0,1),
      col="black", ylab = "Density",
      main = "Density normal") 
abline(h=seq(0,1, by = .1), 
       lty = 3, lwd = .3)

```
]


---
## Normal distribution (dissection)

.pull-left[
```{r, fig.width=5,fig.height=5}
curve(expr = dnorm(x = x, mean=0,sd=1), 
      from = -5, to = 5, ylim = c(0,1),
      col="black", ylab = "Density",
      main = "Density normal") 
abline(h=seq(0,1, by = .1), 
       lty = 3, lwd = .3)

```
]

.pull-right[
For the density distribution, the *area* under the curve between two points is the *probability*. For example, the 95% probability (red region), and 2 tails 2.5% (blue region).

```{r, echo=FALSE, fig.width=5,fig.height=5}
x <- seq(-5, 5, 0.1)
cex  = .7
plot(x, dnorm(x, 0, 1), main = "Density normal", type = "l", lwd = 3, col = "black", ylab = "", xlab = "x", ylim = c(0,1))
# quantile.normal = qnorm(seq(0,1, by = .05))
quantile.normal = qnorm(c(.999,0.001,.975,.025, .95, .05, .9, .1))
abline(v = quantile.normal[!is.infinite(quantile.normal)], lty = 3, lwd = .3) 
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

p = 0.025 

# add the polygon to the left  
lb <- min(x) # Lower bound
ub <- qnorm(p)   # Upper bound
x2 <- seq(min(x), ub, length = 100) # New Grid
y <- dnorm(x2, 0, 1) # Densitypolygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
polygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
text(x = ub, y = p+0.04,labels = paste0("pnorm(qnorm(p)) = \n",p*100,"%"),adj = 0,pos = 2, cex=cex)

text(x = ub-.6, y = .2,labels = paste0("qnorm(p) = ", round(qnorm(p),2)),adj = 0,pos = 1,offset = -1, cex=cex)
arrows(x0 = ub,x1 = ub, y0 = .2,y1 = .1,code = 2,length=.1)


# add the polygon to the right 
lb <- qnorm(1-p) # Lower bound
ub <- max(x)   # Upper bound
x2 <- seq(lb, max(x), length = 100) # New Grid
y <- dnorm(x2, 0, 1) # Density
polygon(c(lb, x2, ub), c(0,y,0), col = rgb(0, 0, 1, alpha = 0.5))
text(x = lb, y = p+0.04,labels = paste0(p*100,"%"),adj = 0,pos = 4, cex=cex)

text(x = lb+.7, y = .2,labels = paste0(round(qnorm(1-p),2)," = qnorm(1-p)"),adj = 0,pos = 1,offset = -1, cex=cex)
arrows(x0 = lb,x1 = lb, y0 = .2,y1 = .1,code = 2,length=.1)

# Add the middle (red) polygon 
lb <- qnorm(p) # Lower bound
ub <- qnorm(1-p)   # Upper bound
x2 <- seq(lb, ub, length = 100) # New Grid
y <- dnorm(x2, 0, 1) # Density
polygon(c(lb, x2, ub), c(0,y,0), col = rgb(1, 0, 0, alpha = 0.5))
text(x = mean(x2), y = .2,labels = paste0((1-2*p)*100,"% \n=", "\npnorm(qnorm(1-p)) - \npnorm(qnorm(p))"),adj = 0,pos = 1, cex=cex)

text(x = 0, y = .55,labels = paste0(quote("Mean")),adj = 0,pos = 1,offset = 0, cex=cex)
arrows(x0 = 0,x1 = 0, y0 = .5,y1 = .42,code = 2,length=.1)

legend("topright",legend = c("Density normal (dnorm)","Density rnorm(100)"), lwd = 1, lty = 1, col = c("black", "green"))

set.seed(123)
rndat =rnorm(100)
# mean(rndat)
dens.nor = density(rndat)
lines(dens.nor, lwd = 3,col = scales::alpha("green",.8))
mybins=hist(rndat, plot = F, density = T, breaks = 100)
crn = mybins$density
brn = mybins$breaks
# # par(new=TRUE)
# # hist(rndat,axes = F, ylab = "", xlab = "", main = "", col = NA, border = NA)
# for (i in 1:c(length(brn)-1)) {
#   x = mean(brn[i],brn[i+1])
#   y = crn[i]
#   points(x,y, col = scales::alpha("black",.2), pch = 19)
# }

# text(x = 2.2, y = .38,labels = paste0("qnorm(1-p)"),adj = 0,pos = 1,offset = 0)
# arrows(x0 = 2.2,x1 = qnorm(1-p), y0 = .35,y1 = .27,code = 2,length=.1)


```
]


---
## Normal distribution (dissection)



---
## Quantile value for a normal distribution 

If X is a random variable distributed with a *Standard normal distribution*, what is the probability of finding X less or equal to $1.645$ or $X\sim N(0,1)$ where $P(X\leq 1.645)$? 
.pull-left[
```{r}
pnorm(1.645)

qnorm(p = 0.05,lower.tail = F)
qnorm(p = 0.025,lower.tail = F)
```
]

---
## Quantile value for a normal distribution 

If X is a random variable distributed with a *Standard normal distribution*, what is the probability of finding X less or equal to $1.645$ or $X\sim N(0,1)$ where $P(X\leq 1.645)$? 
.pull-left[
```{r}
pnorm(1.645)

qnorm(p = 0.05,lower.tail = F)
qnorm(p = 0.025,lower.tail = F)
```
]

.pull-right[
```{r, echo=FALSE, fig.width=5,fig.height=5}
x <- seq(-5, 5, 0.1)
cex  = 1
plot(x, dnorm(x, 0, 1), main = "Density normal", type = "l", lwd = 3, col = "black", ylab = "", xlab = "x", ylim = c(0,1))
quantile.normal = qnorm(c(.95, .05))
abline(v = quantile.normal[!is.infinite(quantile.normal)], lty = 3, lwd = .3) 
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

p = 0.05

# add the polygon to the right 
lb <- qnorm(1-p) # Lower bound
ub <- max(x)   # Upper bound
x2 <- seq(lb, max(x), length = 100) # New Grid
y <- dnorm(x2, 0, 1) # Density
polygon(c(lb, x2, ub), c(0,y,0), col = rgb(1, 0, 0, alpha = 0.5))
text(x = lb, y = p+0.04,labels = paste0(p*100,"%"),adj = 0,pos = 4, cex=cex)

# Add the middle (blue) polygon 
lb <- min(x) # Lower bound
ub <- qnorm(1-p)   # Upper bound
x2 <- seq(lb, ub, length = 100) # New Grid
y <- dnorm(x2, 0, 1) # Density
polygon(c(lb, x2, ub), c(0,y,0), col = rgb(0, 0, 1, alpha = 0.5))
text(x = 0, y = .2,labels = paste0((1-p)*100,"%"),adj = 0,pos = 1, cex=cex)

text(x = 0, y = .55,labels = paste0(quote("Mean")),adj = 0,pos = 1,offset = 0, cex=cex)
arrows(x0 = 0,x1 = 0, y0 = .5,y1 = .42,code = 2,length=.1)
```
]


---
## Normal distribution 

```{r normal_area_function, echo=FALSE, eval=TRUE}
draw.normal <- function(mean = 0, sd = 1, set.seed=1, prob = 0.025, text = FALSE, text.height = .55, where = c("both","left","right")) {
  set.seed(set.seed)
  x <- seq(-5, 5, 0.1)
  cex = 1
  plot(x, dnorm(x, mean, sd), 
       # main = "Density normal", 
       main = "", 
       type = "l", lwd = 3, col = "black", ylab = "", xlab = "x", ylim = c(0,1))
  # abline(v = quantile.normal[!is.infinite(quantile.normal)], lty = 3, lwd = 1, col = c("black")) 
  
  # Horizontal 
  abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)
  
  p = prob 
  
  if(where=="both"){
  # add the polygon to the left  
  lb <- min(x) # Lower bound
  ub <- qnorm(p)   # Upper bound
  x2 <- seq(min(x), ub, length = 100) # New Grid
  y <- dnorm(x2, 0, 1) # Densitypolygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
  polygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
  text(x = -2, y = .2,
       labels = paste0(p*100,"%"),adj = 0,pos = 2, cex=cex)
  
  if (text) {
    text(x = ub, y = .2,
         labels = paste0(round(qnorm(p),2)),adj = 0,pos = 1,offset = 0, cex=cex)
    arrows(x0 = ub,x1 = ub, y0 = .2,y1 = .1,code = 2,length=.1)
  }
  
  # add the polygon to the right 
  lb <- qnorm(1-p) # Lower bound
  ub <- max(x)   # Upper bound
  x2 <- seq(lb, max(x), length = 100) # New Grid
  y <- dnorm(x2, 0, 1) # Density
  polygon(c(lb, x2, ub), c(0,y,0), col = rgb(0, 0, 1, alpha = 0.5))
  text(x = 2, y = .2,
       labels = paste0(p*100,"%"),adj = 0,pos = 4, cex=cex)
  
  if (text) {
    text(x = lb, y = .2,
         labels = paste0(round(qnorm(1-p),2)),adj = 0,pos = 1,offset = 0, cex=cex)
    arrows(x0 = lb,x1 = lb, y0 = .2,y1 = .1,code = 2,length=.1)
  }
  
  # Add the middle (red) polygon 
  lb <- qnorm(p) # Lower bound
  ub <- qnorm(1-p)   # Upper bound
  x2 <- seq(lb, ub, length = 100) # New Grid
  y <- dnorm(x2, 0, 1) # Density
  polygon(c(lb, x2, ub), c(0,y,0), col = rgb(1, 0, 0, alpha = 0.5))
  text(x = mean(x2), y = text.height,
       labels = paste0((1-2*p)*100,"%"),adj = 0,pos = 1, cex=cex)
  
  if (text) {
    text(x = 0, y = .55,
         labels = paste0(quote("Mean")),adj = 0,pos = 1,offset = 0, cex=cex)
    arrows(x0 = 0,x1 = 0, y0 = .5,y1 = .42,code = 2,length=.1)
  }
}
  
  if (where=="left") {
    # add the polygon to the left  
    lb <- min(x) # Lower bound
    ub <- qnorm(p)   # Upper bound
    x2 <- seq(min(x), ub, length = 100) # New Grid
    y <- dnorm(x2, 0, 1) # Densitypolygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
    polygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
    text(x = -2, y = .2,
         labels = paste0(p*100,"%"),adj = 0,pos = 2, cex=cex)
    
    if (text) {
      text(x = ub, y = .2,
           labels = paste0(round(qnorm(p),2)),adj = 0,pos = 1,offset = 0, cex=cex)
      arrows(x0 = ub,x1 = ub, y0 = .2,y1 = .1,code = 2,length=.1)
    }
    
    # Add the middle (red) polygon 
    lb <- qnorm(p) # Lower bound
    ub <- max(x)   # Upper bound
    x2 <- seq(lb, ub, length = 100) # New Grid
    y <- dnorm(x2, 0, 1) # Density
    polygon(c(lb, x2, ub), c(0,y,0), col = rgb(1, 0, 0, alpha = 0.5))
    text(x = 2, y = .2,
         labels = paste0((1-p)*100,"%"),adj = 0,pos = 4, cex=cex)
    
    if (text) {
      text(x = 0, y = .55,
           labels = paste0(quote("Mean")),adj = 0,pos = 1,offset = 0, cex=cex)
      arrows(x0 = 0,x1 = 0, y0 = .5,y1 = .42,code = 2,length=.1)
    } 
    
  }
  
  if (where=="right"){
    # add the polygon to the left  
    lb <- min(x) # Lower bound
    ub <- qnorm(1-p)   # Upper bound
    x2 <- seq(min(x), ub, length = 100) # New Grid
    y <- dnorm(x2, 0, 1) # Densitypolygon(c(lb, x2, ub), c(0, y, 0), col = rgb(0, 0, 1, alpha = 0.5))
    polygon(c(lb, x2, ub), c(0, y, 0), col = rgb(1, 0, 0, alpha = 0.5))
    text(x = -2, y = .2,
         labels = paste0((1-p)*100,"%"),adj = 0,pos = 2, cex=cex)
    
    if (text) {
      text(x = ub, y = .2,
           labels = paste0(round(qnorm(p),2)),adj = 0,pos = 1,offset = 0, cex=cex)
      arrows(x0 = ub,x1 = ub, y0 = .2,y1 = .1,code = 2,length=.1)
    }
    
    # Add the middle (red) polygon 
    lb <- qnorm(1-p) # Lower bound
    ub <- max(x)   # Upper bound
    x2 <- seq(lb, ub, length = 100) # New Grid
    y <- dnorm(x2, 0, 1) # Density
    polygon(c(lb, x2, ub), c(0,y,0), col = rgb(0, 0, 1, alpha = 0.5))
    text(x = 2, y = .2,
         labels = paste0((p)*100,"%"),adj = 0,pos = 4, cex=cex)
    
    if (text) {
      text(x = 0, y = .55,
           labels = paste0(quote("Mean")),adj = 0,pos = 1,offset = 0, cex=cex)
      arrows(x0 = 0,x1 = 0, y0 = .5,y1 = .42,code = 2,length=.1)
    }
    
  }  
}
```

.tiny[
```{r normal_dist_area, fig.width=9,fig.height=6}
par(mfrow=c(2,3), mar = c(4,4,1,1), cex = 1.1)
# The function is not shown, but can be found in the script (Markdown)
draw.normal(where = "both",  prob = 0.05/2)
draw.normal(where = "both",  prob = 0.2/2 )
draw.normal(where = "both",  prob = 0.5/2 )
draw.normal(where = "both",  prob = 0.95/2)
draw.normal(where = "left",  prob = 0.05  )
draw.normal(where = "right", prob = 0.05  )
```
]

---
## Normal distribution 

Find the probability of finding data relative to a standard deviation number 

```{r}
sd = 1
probability.left.side = (pnorm(q = -c(sd*1,sd*2,sd*3),lower.tail = T)*100)
probability.right.side = (pnorm(q =  c(sd*1,sd*2,sd*3),lower.tail = T)*100)
percent.data.under.curve = probability.right.side - probability.left.side
round(percent.data.under.curve,2)

qnorm(p = c(.75,.975, .995),lower.tail = T)
```


---
## Convert a Normal dist. to Standard normal 

- If your data is normally distributed (i.e., $X \sim N(\mu, \sigma)$), you can convert the data to be $X \sim N(0, 1)$ with a scaling
$$Z = \frac{(X-\mu)}{\sigma} $$ 

```{r, echo=FALSE, fig.width=10,fig.height=5}
par(mfrow = c(1,2))
set.seed(1235)
x <- seq(-5, 20, 0.1)
cex  = 1
black.5 = scales::alpha("black",.5)
red.5 = scales::alpha("red",.5)
blue.5 = scales::alpha("blue",.5)
# Add the standard normal 
plot(x, dnorm(x, 0, 1), main = "Density normal", type = "l", lwd = 3, col = black.5, ylab = "", xlab = "x", ylim = c(0,1))
text(x = 0, y = .62,labels = paste0("Standard \nnormal"),adj = 0,pos = 1,offset = 0, cex=cex)
arrows(x0 = 0,x1 = 0, y0 = .5,y1 = .42,code = 2,length=.1)

# Add a population distribution 
lines(x, dnorm(x, 15, 2), main = "Density normal", type = "l", lwd = 3, col = red.5, ylab = "", xlab = "x", ylim = c(0,1))
text(x = 15, y = .55,labels = paste0("Normal \n mean=15 \n sd=2"),adj = 0,pos = 1,offset = 0, cex=cex)
arrows(x0 = 15,x1 = 15, y0 = .35,y1 = .25,code = 2,length=.1)

# Add simulated data 
my.data = rnorm(100, 15, 2)
mean.data = mean(my.data)
sd.data = sd(my.data)
lines(density(my.data), col = blue.5, lwd = 3)
# Add guides
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

# Add the standard normal 
plot(x, dnorm(x, 0, 1), main = "Density normal", type = "l", lwd = 3, col = black.5, ylab = "", xlab = "x", ylim = c(0,1))

# Add a population distribution 
lines(x, dnorm(x, 0, 1), main = "Density normal", type = "l", lwd = 3, col = red.5, ylab = "", xlab = "x", ylim = c(0,1))
lines(density((my.data-mean.data)/sd.data), col = blue.5, lwd = 3)

# Add guides
abline(h=seq(0,1, by = .1), lty = 3, lwd = .3)

```




---
## Challenge 3 ![:cube]()

Let's say you want to simulate the length of different beak sizes for 10 birds. 

Use `rnorm()` to generate 10 random normal numbers. 

```{r, eval=FALSE}
rnorm()
```


---
## Challenge 3 - Solution ![:cube]()

Let's say you want to simulate the length of different beak sizes for 10 birds. 

Use `rnorm()` to generate 10 random normal numbers. 

```{r}
set.seed(1234)
n <-10
rnorm(n)
```

- Why are there negative values? Because the mean = 0, so some values spill over the mean on both sides, and in the negative numbers. 


###############
---
class: inverse, center, middle

# Simulating data for power analysis

---
## Power analysis

For the moment, please refer to [this link](https://www.r-bloggers.com/2020/05/power-analysis-by-data-simulation-in-r-part-ii/).

.tiny[
```{r, fig.width=3,fig.height=3}
set.seed(1234)
n = 30
group1 <- rnorm(n = n, mean = 1, sd = 2)
group2 <- rnorm(n = n, mean = 0, sd = 2)
hist(group1, breaks = 10, main = "Histogram of both groups", xlab = "")
hist(group2, add = TRUE, breaks = 10, col= scales::alpha("blue",.5))
t.test(group1, group2, paired = FALSE, var.equal = TRUE, conf.level = 0.9)

set.seed(1)
n_sims <- 10 # we want 1000 simulations
p_vals <- c()
power_at_n <- c(0) # this vector will contain the power for each sample-size (it needs the initial 0 for the while-loop to work)
cohens_ds <- c()
cohens_ds_at_n <- c() 
n <- 30 # sample-size 
i <- 2
while(power_at_n[i-1] < .95){
  for(sim in 1:n_sims){
    group1 <- rnorm(n,1,2) # simulate group 1
    group2 <- rnorm(n,0,2) # simulate group 2
    p_vals[sim] <- t.test(group1, group2, paired = FALSE, var.equal = TRUE, conf.level = 0.9)$p.value # run t-test and extract the p-value
    cohens_ds[sim] <- abs((mean(group1)-mean(group2))/(sqrt((sd(group1)^2+sd(group2)^2)/2))) # we also save the cohens ds that we observed in each simulation
  }
  power_at_n[i] <- mean(p_vals < .10) # check power (i.e. proportion of p-values that are smaller than alpha-level of .10)
  cohens_ds_at_n[i] <- mean(cohens_ds) # calculate means of cohens ds for each sample-size
  n <- n+1 # increase sample-size by 1
  i <- i+1 # increase index of the while-loop by 1 to save power and cohens d to vector
}
power_at_n <- power_at_n[-1] # delete first 0 from the vector
cohens_ds_at_n <- cohens_ds_at_n[-1] # delete first NA from the vector
plot(30:(n-1), power_at_n, xlab = "Number of participants per group", ylab = "Power", ylim = c(0,1), axes = TRUE)
abline(h = .95, col = "red")

plot(30:(n-1), cohens_ds_at_n, xlab = "Number of participants per group", ylab = "Cohens D", ylim = c(0.45,0.55), axes = TRUE)
abline(h = .50, col = "red")
```
]


###############
---
class: inverse, center, middle

# Simulating data for models

---
## Linear model refresher

$$Y = \beta_{0} + \beta_{1} x_{1} + \cdots + \beta_{p} x_{p} + \epsilon$$

- $Y$ is the response variable
- $\beta_0$ is the intercept
- $\beta_1$ is the coefficient of variation for the first explanatory variable ($x_1$)
- $\beta_p$ is the coefficient of variation for the $p^{th}$ explanatory variable for the $p^{th}$ $x_p$ explanatory variable
- $\epsilon$ is the residual of the model. Note that $\epsilon \sim N(\mu=0,sd = \sigma)$
- The goal is to find the **best estimation** of the parameters ($\beta$s), while minimizing the residuals, and assess the goodness of fit of the model

---
## Generalized linear model refresher

| Type | Equation |
| ---- | -------- |
| Linear | $Y = \beta_{0} + \beta_{1} x_{1}  + \epsilon$ |
| Poisson | $Y \sim Poisson(\mu)$ with $\text {ln} \mu=\beta_0+\beta_1x$ or $\mu=e^{\beta_0+\beta_1x}$ (no separate error term as $\lambda$ determines both the mean and variance) |
| Logistic | $Y \sim Binomial(p)$ with $\text{log} \Bigl(\frac{p}{1-p}\Bigr) = \beta_0 +\beta_1x$ or $p=\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$ where $\text{log} \Bigl(\frac{p}{1-p}\Bigr)$ is the log odds or log likelihood. The Y values are determined by a Bernoulli distribution (binomial of size = 1) |

<!-- 
In GLM, we don'T model the individual Y values, but the mean
https://www.theanalysisfactor.com/generalized-linear-models-no-error-term/
In other words "there's no common error distribution independent of predictor values"
see https://stats.stackexchange.com/questions/124818/logistic-regression-error-term-and-its-distribution
-->
<!-- For poisson (there is no 'error' term like linear regression, since there is inherent variability)?? -->
<!-- logistic regression there is an error (or randomness hidden in the Bernoulli (or binomial(size =1)) with probability p)  -->
<!-- Simulate logistic regression https://stats.stackexchange.com/questions/46523/how-to-simulate-artificial-data-for-logistic-regression/46525 -->

---
## Generalized linear model: Logistic

<!-- Inspiration : https://stats.stackexchange.com/questions/46523/how-to-simulate-artificial-data-for-logistic-regression/46525
-->
```{r sim_logistic}
set.seed(987654)
n = 1000
x1 = rnorm(n = n, mean = 6, sd = 1)
x2 = rnorm(n = n, mean = 0, sd = 1)
# Rescale the data
x1z = scale(x1)
x2z = scale(x2)
z = 1 + 2*x1z + 3*x2
pr = 1/(1+exp(-z)) # inverse-logit function; Note that 1/(1+exp(-x))== exp(x)/(1+exp(x)), same as pr2 = boot::inv.logit(z)
y = rbinom(n = n, size = 1, prob = pr) # Bernoulli response variable (which is a special case of the binomial with size =1 )

# Combine the data in a dataframe 
df = data.frame(y = y, x1 = x1, x2 = x2)
```

---
## Generalized linear model: Logistic

```{r sim_logistic_glm, fig.width=5,fig.height=5}
#now feed it to glm:
glm.logist = glm( y~x1+x2, data=df, family="binomial")
plot(y~x1, data = df, col = scales::alpha("black",.5), pch = 19)
newdata <- data.frame(x1=seq(min(x1), max(x1),len=n), x2 = seq(min(x2), max(x2),len=n))
newdata$y = predict(object = glm.logist, newdata = newdata, type = "response") 
lines(x = newdata$x1,
      y = newdata$y, col = "red",lwd = 2)
```

---
## Generalized linear model: Logistic

.tiny[
```{r sim_logistic_glm_3D}
# scatterplot3d::scatterplot3d(x = x1,y = x2,z = y)
library(plotly)
fig <- plot_ly(df, x = ~x1, y = ~x2, z = ~y,
               marker = list(color = ~y, colorscale = c('#FFE1A1', '#683531'), showscale = TRUE))
fig <- fig %>%add_markers() %>% layout(scene = list(xaxis = list(title = 'x1'),
                                   yaxis = list(title = 'x2'),
                                   zaxis = list(title = 'y')),
                      annotations = list(x = 1.08,y = 1.05,
                        text = 'Scale', xref = 'paper',yref = 'paper',
                        showarrow = FALSE)) 
fig
```
]

---
## Simulate categories (t-test, Anova)
<!-- Although this is with only 2 groups, I'm showing this as a quick example -->
.pull-left[
```{r}

set.seed(1234); n = 1000
y1 = rnorm(n, mean = 15, sd = 1)
y2 = rnorm(n, mean = 15.5, sd = 1)

sim.aov1 = data.frame(y = y1, gr = "A")
sim.aov2 = data.frame(y = y2, gr = "B")
df.aov = rbind(sim.aov1, sim.aov2)
df.aov$gr = factor(df.aov$gr)

# t.test(y~gr, data = df.aov) or
aov.out = aov(y~gr, data = df.aov)
#summary(aov.out)
tk.test = TukeyHSD(aov.out)
round(tk.test$gr,2)
```
]

.pull-right[
```{r, fig.width=5,fig.height=5}
plot(y~gr, data = df.aov)
```

]

---
## Linear mixed model refresher

- A simple linear model is actually the 'simplest' mixed model (although the convention is that we don't called it mixed)
- What is the random effect in a linear model? 

$$Y = \beta_{0} + \beta_{1} x_{1} + \cdots + \beta_{p} x_{p} + \epsilon$$

---
## Linear mixed model refresher

- A simple linear model is actually the 'simplest' mixed model (although the convention is that we don't called it mixed)
- What is the random effect in a linear model? 

$$Y = \beta_{0} + \beta_{1} x_{1} + \cdots + \beta_{p} x_{p} + \epsilon$$

- The residual of the model ($\epsilon$) is actually the 'random effect' since it is drawn from a distribution which can change for each data point $\epsilon \sim N(\mu=0,sd = \sigma)$. 

---
## Simulating a linear mixed model 

For now, please see [this link](https://debruine.github.io/tutorials/sim-lmer.html). 

```{r}
library(lmerTest)
n = 20
sd.n = 2
# Generate dataframe 
x = 1:n
values = rnorm(n = n,mean = 0,sd = sd.n)
gr = rep(c("short","tall"), each = n/2)
sim.df = data.frame(x,values,gr)

plot(density(sim.df[sim.df$gr%in%"short","values"]), col = "black", ylim = c(0,1), main = "Density")
lines(density(sim.df[sim.df$gr%in%"tall","values"]), col = "red")
legend("toprigh",legend = c("Short","Tall"),col = c("black","red"), lty = 1)
```


---
## Advanced simulations

- Now that we have the building blocks to make simulations, we are going to add another layer:
- We want to simulate a process and then see how our simulation responds to a fluctuation in a parameter
- For this, we need a certain workflow that resembles this: 
  1. create a blank vector in which a value of interest will be stored in 
  2. Within a loop or many `for(){}` loops, we are going to simulate data, get a desired result and store the result for future analysis 
  3. Summarize our findings (in a plot, with calculations, etc.) 

.alert[ATTENTION]. This is an iterative process. Usually, we make small increments writing up the code and then put it all together in a cohesive function that will make exactly what we are looking for. 


---
## Advanced simulations
.tiny[
```{r}
# Defining the population 
n = 600 # Number of elements to be generated 
set.seed(13) # Set RNG 
x = rnorm(n) # Generate numbers from normal distribution 
reedddd = scales::alpha("blue",.4) # Make colour transparent 

# Definte the function 
sample.mean.pop.est <- function(x,n, sample.size, ylim = NULL) {
  # x: is the actual values of the trait measured 
  # n: size of the population (number of individuals or items)
  # sample.size: how big is the sample size from which the MEAN will be calculated from 
  # ylim: add a maximum if needed 
  # histogram of the population 
  
  # Just get the stats from the histogram 
  pop.hist = hist(x, plot = F) # Make base histogram 
  
  # Make empty vector
  tmp.v = c(NA) 
  
  # For loop to calculate the mean based on a sample from the population 
  for (i in 1:n) {
    tmp = sample(x = x, size = sample.size, replace = FALSE)
    # Record that information (mean of the sample)
    tmp.v[i] = mean(tmp)
  } # End i
  
  # Sample histogram 
  sample.hist = hist(tmp.v, plot = F)
  # Population histogram 
  hist(x, ylim = range(c(0,c(sample.hist$counts, pop.hist$counts), ylim)), 
       main = paste("Sample n =", sample.size))
  # Add the sample estimate 
  sample.hist = hist(tmp.v, col = reedddd, add=T)
} # End sample.mean.pop.est
```
]

---
## Advanced simulations

.small[
```{r}
par(mfrow=c(2,2), lwd = .3)
sample.mean.pop.est(x = x, n = n, sample.size = 1, ylim = 300)
sample.mean.pop.est(x = x, n = n, sample.size = 10, ylim = 300)
sample.mean.pop.est(x = x, n = n, sample.size = 50, ylim = 300)
sample.mean.pop.est(x = x, n = n, sample.size = 500, ylim = 300)
```
]

---
## References
.small[
https://web.stanford.edu/class/bios221/labs/simulation/Lab_3_simulation.html
https://bookdown.org/rdpeng/rprogdatascience/simulation.html
https://towardsdatascience.com/statistical-simulation-in-r-part-1-d9cb4dc393c9
https://www.r-bloggers.com/2020/05/power-analysis-by-data-simulation-in-r-part-ii/
https://it.unt.edu/simple-data-simulations
https://www.youtube.com/watch?v=tvv4IA8PEzw
http://www.columbia.edu/~cjd11/charles_dimaggio/DIRE/resources/R/simRreg.pdf
https://aosmith.rbind.io/2018/08/29/getting-started-simulating-data/
https://www.crumplab.com/programmingforpsych/simulating-and-analyzing-data-in-r.html
https://users.phhp.ufl.edu/rlp176/Courses/PHC6089/R_notes/simulations.html
https://nickch-k.github.io/introcausality/Cheat%20Sheets/Simulation_Cheat_Sheet.pdf
https://debruine.github.io/tutorials/sim-lmer.html
]

---
class: inverse, center, bottom

# Thank you for attending this workshop!

![:scale 50%](images/qcbs_logo.png)


